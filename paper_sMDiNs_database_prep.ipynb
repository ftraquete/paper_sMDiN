{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee0e534d",
   "metadata": {},
   "source": [
    "# Sample Mass-Difference Networks in Metabolomics Data Analysis\n",
    "\n",
    "Notebook to support the study on the application of **Sample M**ass-**Di**fference **N**etworks as a highly specific competing form of pre-processing procedure for high-resolution metabolomics data.\n",
    "\n",
    "Mass-Difference Networks are focused into making networks from a list of masses. Each _m/z_ will represent a node. Nodes will be connected if the difference in their masses can be associated to a simple chemical reaction (enzymatic or non-enzymatic) that led to a change in the elemental composition of its metabolite.\n",
    "\n",
    "The set of mass differences used to build said networks are called a set of MDBs - Mass-Difference-based Building block.\n",
    "\n",
    "This is notebook `paper_sMDiNs_database_prep.ipynb`.\n",
    "\n",
    "\n",
    "## Organization of the Notebook\n",
    "\n",
    "- **Set up database of the benchmark datasets**\n",
    "- Description of the benchmark datasets\n",
    "- **Building transformation list (of MDBs) and setting up list of masses files to use in Cytoscape's MetaNetter (to build MDiNs)**\n",
    "- Show the benchmark datasets characteristics\n",
    "- Explanation of the parameters used in Cytoscape's MetaNetter 2.0 to build the MDiNs.\n",
    "- Apply intensity-based pre-treatments.\n",
    "- **Store pre-processed and pre-treated data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85e0c49",
   "metadata": {},
   "source": [
    "#### Needed Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44640c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "# Metabolinks package\n",
    "import metabolinks as mtl\n",
    "import metabolinks.transformations as transf\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# json for persistence\n",
    "import json\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe409ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01cea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atomic masses - https://ciaaw.org/atomic-masses.htm\n",
    "#Isotopic abundances-https://ciaaw.org/isotopic-abundances.htm/https://www.degruyter.com/view/journals/pac/88/3/article-p293.xml\n",
    "# Isotopic abundances from Pure Appl. Chem. 2016; 88(3): 293–306,\n",
    "# Isotopic compositions of the elements 2013 (IUPAC Technical Report), doi: 10.1515/pac-2015-0503\n",
    "\n",
    "chemdict = {'H':(1.0078250322, 0.999844),\n",
    "            'C':(12.000000000, 0.988922),\n",
    "            'N':(14.003074004, 0.996337),\n",
    "            'O':(15.994914619, 0.9976206),\n",
    "            'Na':(22.98976928, 1.0),\n",
    "            'P':(30.973761998, 1.0),\n",
    "            'S':(31.972071174, 0.9504074),\n",
    "            'Cl':(34.9688527, 0.757647),\n",
    "            'F':(18.998403163, 1.0),\n",
    "            'C13':(13.003354835, 0.011078) # Carbon 13 isotope\n",
    "           } \n",
    "\n",
    "# electron mass from NIST http://physics.nist.gov/cgi-bin/cuu/Value?meu|search_for=electron+mass\n",
    "electron_mass = 0.000548579909065"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f4d72",
   "metadata": {},
   "source": [
    "## Description of dataset records\n",
    "\n",
    "`datasets` is the global dict that holds all data sets. It is a **dict of dict's**.\n",
    "\n",
    "Each data set is **represented as a dict**.\n",
    "\n",
    "Each record has the following fields (keys):\n",
    "\n",
    "- `name`: the table/figure name of the data set\n",
    "- `source`: the biological source for each dataset\n",
    "- `mode`: the aquisition mode\n",
    "- `alignment`: the alignment used to generate the data matrix\n",
    "- `data`: the data matrix\n",
    "- `target`: the sample labels, possibly already integer encoded\n",
    "- `MDiN`: Mass-Difference Network - Not present here, only on sMDiNsAnalysis notebook\n",
    "- `<treatment name>`: transformed data matrix / network. These treatment names can be\n",
    "    - `Ionly`: missing value imputed data by 1/5 of the minimum value in each sample in the dataset, only\n",
    "    - `NGP`: normalized, glog transformed and Pareto scaled\n",
    "    - `Ionly_RF`: missing value imputed data by random forests, only\n",
    "    - `NGP_RF`: normalized, glog transformed and Pareto scaled\n",
    "    - `IDT`: `NGP_RF` or `NGP` - Intensity-based Data pre-Treatment chosen as comparison based on which of the two performed better for each dataset and each statistical method\n",
    "    - `sMDiN`: Sample Mass-Difference Networks - Not present here, only on sMDiNsAnalysis notebook\n",
    "       \n",
    "- `<sMDiN analysis name>`: data matrix from nework analysis of MDiNs - Not in this notebook\n",
    "    - `Degree`: degree analysis of each sMDiN\n",
    "    - `Betweenness`: betweenness centrality analysis of each sMDiN\n",
    "    - `Closeness`: closeness centrality of analysis of each sMDiN\n",
    "    - `MDBI`: analysis on the impact of each MDB (Mass-Difference based building-block) on building each sMDiN\n",
    "    - `GCD11`: Graphlet Correlation Distance of 11 different orbits (maximum of 4-node graphlets) between each sMDiN.\n",
    "    - `WMDBI`: an alternative calculation of MDBI using the results from the degree analysis.\n",
    "\n",
    "- `iter_fold_splits`: contains nested dicts that identify and contain each transformed training and testing groups data matrices with their respective iteration, training/test, fold number and one of the previously mentioned data pre-treatments\n",
    "- `train`: specific to the HD dataset; contains a set of the different pre-treatments and sMDin analysis mentioned and a target based on the training set defined for HD\n",
    "- `test`: specific to the HD dataset; contains a set of the different pre-treatments and sMDin analysis mentioned and a target based on the external test set defined for HD\n",
    "\n",
    "\n",
    "The keys of `datasets` may be shared with dicts holding records resulting from comparison analysis.\n",
    "\n",
    "Here are the keys (and respective names) of datasets used in this study:\n",
    "\n",
    "- GD_neg_global2 (GDg2-)\n",
    "- GD_neg_class2 (GDc2-)\n",
    "- YD (YD)\n",
    "- vitis_types (GD types)\n",
    "- HD (HD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a60ea",
   "metadata": {},
   "source": [
    "### Description of grapevine data sets\n",
    "\n",
    "Grapevine Dataset - 33 samples belonging to 11 different grapevine varieties (3 samples per variety/biological group) of FT-ICR-MS metabolomics data obtained in electrospray negative ionization mode.\n",
    "\n",
    "5 different _Vitis_ species (other than _V. vinifera_) varieties:\n",
    "\n",
    "- CAN - 3 Samples (14, 15, 16) of _V. candicans Engelmann_ (VIVC variety number: 13508)\n",
    "- RIP - 3 Samples (17, 18, 19) of _V. riparia Michaux_ (Riparia Gloire de Montpellier, VIVC variety number: 4824) \n",
    "- ROT - 3 Samples (20, 21, 22) of _V. rotundifolia_ (Muscadinia Rotundifolia Michaux cv. Rotundifolia, VIVC variety number: 13586)\n",
    "- RU - 3 Samples (35, 36, 37) of _V. rupestris Scheele_ (Rupestris du lot, VIVC variety number: 10389)\n",
    "- LAB - 3 Samples (8, 9, 10) of _V. labrusca_ (Isabella, VIVC variety number: 5560)\n",
    "\n",
    "6 different _V. vinifera_ cultivars varieties are:\n",
    "\n",
    "- SYL - 3 samples (11, 12, 13) of the subspecies _sylvestris_ (VIVC variety number: -)\n",
    "- CS - 3 Samples (29, 30, 31) of the subspecies _sativa_ cultivar Cabernet Sauvignon (VIVC variety number: 1929)\n",
    "- PN - 3 Samples (23, 24, 25) of the subspecies _sativa_ cultivar Pinot Noir (VIVC variety number: 9279)\n",
    "- REG - 3 Samples (38, 39, 40) of the subspecies _sativa_ cultivar Regent (VIVC variety number: 4572)\n",
    "- RL - 3 Samples (26, 27, 28) of the subspecies _sativa_ cultivar Riesling Weiss (VIVC variety number: 10077)\n",
    "- TRI - 3 Samples (32, 33, 34) of the subspecies _sativa_ cultivar Cabernet Sauvignon (VIVC variety number: 15685)\n",
    "\n",
    "Data acquired by Maia et al. (2020):\n",
    "\n",
    "- Maia M, Ferreira AEN, Nascimento R, et al. Integrating metabolomics and targeted gene expression to uncover potential biomarkers of fungal / oomycetes ‑ associated disease susceptibility in grapevine. Sci Rep. Published online 2020:1-15. doi:10.1038/s41598-020-72781-2\n",
    "- Maia M, Figueiredo A, Silva MS, Ferreira A. Grapevine untargeted metabolomics to uncover potential biomarkers of fungal/oomycetes-associated diseases. 2020. doi:10.6084/m9.figshare.12357314.v1\n",
    "\n",
    "**Peak Alignment** and **Peak Filtering** were performed with function `metabolinks.peak_alignment.align()`. Human leucine enkephalin (Sigma Aldrich) was used as the reference feature (internal standard, [M+H]+ = 556.276575 Da or [M-H]- = 554.262022 Da).\n",
    "\n",
    "**2** data matrices were constructed from this data:\n",
    "\n",
    "- `GD_neg_global2` (GDg2-) was generated after retaining only features that occur (globally) at least twice in all 33 samples of the dataset (filtering/alignment) for the **negative mode** data acquisition.\n",
    "- `GD_neg_class2` (GDc2-) was generated after retaining only features that occur at least twice in the three replicates of at least one _Vitis_ variety in the dataset (filtering/alignment) for the **negative mode** data acquisition.\n",
    "\n",
    "For the purpose of assessing the performance of supervised methods each of these four datasets was used with target labels defining classes corresponding to replicates of each of the 11 Vitis species/cultivars.\n",
    "\n",
    "For the purpose of assessing the performance of supervised methods under a binary (two-class) problem, data set `GD_neg_class2` was also used with target labels defining two classes: Vitis vinifera cultivars and \"wild\", non-vinifera Vitis species. This is dataset `vitis_types` (GD types)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74ce5b",
   "metadata": {},
   "source": [
    "### Description of the yeast data set\n",
    "\n",
    "Yeast dataset - 15 samples belonging to 5 different yeast strains of _Saccharomyces cerevisiae_ (3 biological replicates per strain/biological group) of FT-ICR-MS metabolomics data obtained in positive ionization mode. The 5 strains were: the reference strain BY4741 (represented as BY) and 4 single-gene deletion mutants of this strain – ΔGLO1 (GLO1), ΔGLO2 (GLO2), ΔGRE3 (GRE3) and ΔENO1 (ENO1). These deleted genes are directly or indirectly related to methylglyoxal metabolism.\n",
    "\n",
    "Data acquired by Luz et al. (2021):\n",
    "\n",
    "- Luz J, Pendão AS, Silva MS, Cordeiro C. FT-ICR-MS based untargeted metabolomics for the discrimination of yeast mutants. 2021. doi:10.6084/m9.figshare.15173559.v1\n",
    "\n",
    "**Peak Alignment** and **Peak Filtering** was performed with MetaboScape 4.0 software (see reference above for details in sample preparation, pre-processing, formula assignment). In short, Yeast Dataset was obtained with Electrospray Ionization in Positive Mode and pre-processed by MetaboScape 4.0 (Bruker Daltonics). Human leucine enkephalin (Sigma Aldrich) was used as the reference feature (internal standard, [M+H]+ = 556.276575 Da or [M-H]- = 554.262022 Da).\n",
    "\n",
    "A data matrix was constructed from this data: data set named **`YD` (YD)** was generated after retaining only features that occur (globally) at least twice in all 15 samples (filtering/alignment).\n",
    "\n",
    "Furthermore, peaks higher than 1000 m/z were filtered out. Formula assignment was performed with MetaboScape 4.0 software and, to consider peaks that had the same formula assigned, these were joined together in a single peak (`feature_filter` function).\n",
    "\n",
    "For the purpose of assessing the performance of supervised methods, this data set was used with target labels defining classes corresponding to replicates of each of the 4 yeast strains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c1f11",
   "metadata": {},
   "source": [
    "#### Loading yeast data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_MetScape_file(filename,\n",
    "                          col_renamer=None,\n",
    "                          add_labels=None,\n",
    "                          remove_ref_feat=None,):\n",
    "    \n",
    "    \"\"\"Read in a MetaboScape bucket table from a CSV file.\"\"\"\n",
    "    \n",
    "    data = pd.read_csv(filename).set_index('Bucket label')\n",
    "    \n",
    "    # optionally rename sample_names\n",
    "    if col_renamer is not None:\n",
    "        data = data.rename(columns=renamer)\n",
    "    \n",
    "    # optionally remove a reference feature (if already normalized)\n",
    "    if remove_ref_feat is not None:\n",
    "        #print(f'Feature {remove_ref_feat}\\n{data.loc[remove_ref_feat, :]}\\n----------')\n",
    "        data = data.drop(index=[remove_ref_feat])\n",
    "        \n",
    "    # Joining rows that have the same formula assigned (with feature_filter) while keeping rows without formulas assigned.\n",
    "    data = feature_filter(data)\n",
    "    \n",
    "    # split in peak metadata and intensities\n",
    "    peak_cols = ['m/z', 'Name', 'Formula']\n",
    "    intensity_cols = [c for c in list(data.columns) if c not in peak_cols]\n",
    "    peaks = data[peak_cols]\n",
    "    intensities = data[intensity_cols]\n",
    "\n",
    "    # replace zeros for NaN's\n",
    "    intensities = intensities.replace(0, np.nan).dropna(how='all')\n",
    "    \n",
    "    # force peaks to have the same features as the (trimmed) intensities\n",
    "    peaks = peaks.reindex(intensities.index)\n",
    "\n",
    "    # optionally, add labels to intensities\n",
    "    if add_labels is not None:\n",
    "        intensities = mtl.add_labels(intensities, labels=add_labels)\n",
    "    \n",
    "    return {'filename': filename,\n",
    "            'peaks':peaks,\n",
    "            'intensities': intensities}\n",
    "\n",
    "def renamer(colname):\n",
    "    # Util to optionally remove all those 00000 from sample names\n",
    "    return ''.join(colname.split('00000'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5967446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_filter(Spectra):\n",
    "    \"\"\"Join all features that share the same formula by addition.\n",
    "       \n",
    "       Spectra: Pandas DataFrame (dataset).\n",
    "       \n",
    "       Returns: Pandas DataFrame with filtered dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    df = Spectra\n",
    "    # Variables to store results\n",
    "    rp = pd.DataFrame(columns = df.columns)\n",
    "    rep = []\n",
    "    # Series with number of times each formula appears in the dataset\n",
    "    form_count = df['Formula'].value_counts()\n",
    "    a = 0\n",
    "    \n",
    "    for i in df['Formula']:\n",
    "        if pd.isnull(i) == True: # If no formula is assigned to the feature, the feature stays in the new dataset\n",
    "            #rp = rp.append(df.iloc[a,:])\n",
    "            rp.loc[df.iloc[a,:].name] = df.iloc[a,:]\n",
    "\n",
    "        else: # If a formula is assigned\n",
    "            if form_count[i] > 1: # If the formula appears more than one time in the dataset\n",
    "                if i not in rep: # And If it didn't already appear before\n",
    "                    \n",
    "                    # Bucket label, name and formula info are the ones from the greater intensity m/z peak (of peaks with the\n",
    "                    # same formula)\n",
    "                    peaks = df.loc[df['Formula'] == i,:].iloc[:,3:].sum(axis = 1)\n",
    "                    peaks_idx = peaks.idxmax()\n",
    "                    info = df.loc[peaks_idx].iloc[:3] # Bucket label, name and formula info\n",
    "                    \n",
    "                    # Addition of all m/z peaks intensity with the same formula\n",
    "                    newrow = df.loc[df['Formula'] == i,:].iloc[:,3:].sum(axis = 0)\n",
    "                    newrow_c = pd.concat([info, newrow], axis = 0) # Join with the m/z peak metadata\n",
    "                    # m/z peak is from the greater intensity m/z peak (of peaks with the same formula)\n",
    "                    newrow_c = newrow_c.rename(df.loc[peaks_idx].name)\n",
    "                    \n",
    "                    # Append peak with intensity added from all peaks sharing the same formula\n",
    "                    #rp = rp.append(newrow_c)\n",
    "                    rp.loc[newrow_c.name] = newrow_c\n",
    "\n",
    "                    rep.append(i) # Append repeating formulas so this process isn't repeated again for the other m/z peaks\n",
    "\n",
    "            else:\n",
    "                # Append peaks with formulas that aren't repeated in the dataset\n",
    "                #rp = rp.append(df.iloc[a,:])\n",
    "                rp.loc[df.iloc[a,:].name] = df.iloc[a,:]\n",
    "                #print(df.iloc[a,:].name)\n",
    "\n",
    "        a = a + 1\n",
    "\n",
    "    # return DataFrame\n",
    "    return rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2457fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels of the 5 biological groups (5 yeast strains) - only added after\n",
    "yeast_classes = 'WT ΔGRE3 ΔENO1 ΔGLO1 ΔGLO2'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the file and keep results in dicts \n",
    "\n",
    "prefix_to_drop = None # change to 'ENO' to remove ENO strain\n",
    "\n",
    "# MetaboScape data\n",
    "MS_data = read_MetScape_file('5yeasts_notnorm.csv', \n",
    "                             remove_ref_feat=None,\n",
    "                             add_labels=None,\n",
    "                             col_renamer=renamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31fab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a065973a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# keep features that appear in at least two samples\n",
    "yeast_datamatrix = transf.keep_atleast(MS_data['intensities'].transpose(), minimum=2)\n",
    "\n",
    "# Changing the index to be numbers - our list of neutral masses from the bucket lists.\n",
    "new_columns = []\n",
    "for i in range(len(yeast_datamatrix.columns)):\n",
    "    new_columns.append(float(yeast_datamatrix.columns[i][:-3]))\n",
    "yeast_datamatrix.columns = new_columns\n",
    "\n",
    "# Only keep peaks below 1000 m/z - Very few peaks above 1000 m/z.\n",
    "yeast_datamatrix = yeast_datamatrix.T[yeast_datamatrix.columns < 1000].T\n",
    "\n",
    "yeast_datamatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e029b4fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# keep features that appear in at least two samples\n",
    "yeast_datamatrix = transf.keep_atleast(MS_data['intensities'].transpose(), minimum=2)\n",
    "\n",
    "# Changing the index to be numbers - our list of neutral masses from the bucket lists.\n",
    "new_columns = []\n",
    "for i in range(len(yeast_datamatrix.columns)):\n",
    "    new_columns.append(float(yeast_datamatrix.columns[i][:-3]))\n",
    "yeast_datamatrix.columns = new_columns\n",
    "\n",
    "# Only keep peaks below 1000 m/z - Very few peaks above 1000 m/z.\n",
    "yeast_datamatrix = yeast_datamatrix.T[yeast_datamatrix.columns < 1000].T\n",
    "\n",
    "yeast_datamatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678aa51",
   "metadata": {},
   "source": [
    "### Description of the Human dataset\n",
    "\n",
    "Human Dataset - 249 samples belonging to 2 different classes of HILIC-MS data obtained in positive ionization mode. The Chromatography system was a Thermo Dionex Ultimate 3000 with the column being a Waters Xbridge BEH HILIC (75 x 2.1mm, 2.5um). Mass Spectrometry was performed in a Thermo Q Exactive HF hybrid Orbitrap operating in positive electrospray ionization mode. Further information in the data deposition site mentioned below.\n",
    "\n",
    "The 2 classes are:\n",
    "\n",
    "- 135 pre-operative blood (serum) samples from patients with **'No Recurrence'** of Prostate Cancer after Radical Prostatectomy\n",
    "- 114 pre-operative blood (serum) samples from patients with **'Recurrence'** of Prostate Cancer after Radical Prostatectomy\n",
    "\n",
    "I think this means samples were collected before the Prostatectomy before knowledge of recurrence or no recurrence? There was a total of 80 subjects.\n",
    "\n",
    "Data taken from data deposition site: https://www.metabolomicsworkbench.org/data/DRCCMetadata.php?Mode=Study&DataMode=AllData&StudyID=ST001082&StudyType=MS&ResultType=5#DataTabs\n",
    "\n",
    "This data is available at the NIH Common Fund's National Metabolomics Data Repository (NMDR) website, the Metabolomics Workbench, https://www.metabolomicsworkbench.org where it has been assigned Project ID PR000724. The data can be accessed directly via it's Project DOI: 10.21228/M83D5V. \n",
    "\n",
    "Data deposited by the Georgia Institute of Technology and researcher Facundo Fernandez. This data was taken already aligned in a 2D numerical data matrix. \n",
    "\n",
    "Data used in the paper: Clendinen CS, Gaul DA, Monge ME, et al. Preoperative Metabolic Signatures of Prostate Cancer Recurrence Following Radical  Prostatectomy. J Proteome Res. 2019;18(3):1316-1327. doi:10.1021/acs.jproteome.8b00926\n",
    "\n",
    "Dataset named **HD (HD)** was generated after retaining only features that occur (globally) at least twice in all samples of the dataset. For the purpose of assessing the performance of supervised methods this dataset was used with target labels defining classes based on which of the 2 classes mentioned prior, the samples belonged to.\n",
    "\n",
    "\n",
    "The data read from the site seemed to be completely untreated, which was perfect for our efforts. We can't be sure but the presence of plenty of missing values (very useful for us) points to no great peak filtering and missing value imputation being made, the bar plot built from adding up all intensities of each sample seems to point that no normalization had yet been employed in the data, and the distribution observed in the boxplot of the intensities of some of the features in the samples seems to point that no scaling had been used yet in the data as well.\n",
    "\n",
    "The data also had 2 copies of each sample, where the 2nd copies were equal to the first ones multiplied by a constant. This constant was different for each sample. We removed the presence of the 2nd copies. The index (metabolites) were reduced from the notation m/z value_retention time to only have the m/z value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2f0cdb",
   "metadata": {},
   "source": [
    "#### Loading Human dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset, this dataset has two copies of the samples.The 2nd copy is equal to the 1st multiplied by a constant\n",
    "# unique to each sample.\n",
    "human_datamatrix_base = pd.read_excel('ST001082_AN001766_HD.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b73f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_datamatrix = human_datamatrix_base.iloc[:-1, :-4] # Just select the rows corresponding to the dataset\n",
    "mz_list = human_datamatrix_base.iloc[:-1, -4] # Select column with list of m/z values\n",
    "\n",
    "human_datamatrix = human_datamatrix.set_index(human_datamatrix.columns[0]) # Set index as the metabolites name\n",
    "human_datamatrix = human_datamatrix.replace({0:np.nan}) # Replacing 0 values as missing values\n",
    "\n",
    "human_datamatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49b8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating m/z values with different retention times\n",
    "\n",
    "#abc = []\n",
    "#for i in list(mz_list):\n",
    "#    if i in abc:\n",
    "#        print(i)\n",
    "#    else:\n",
    "#        abc.append(i)\n",
    "#len(set(list(mz_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a3e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one of the two copies of samples in the dataset by removing the samples ending with '.1'\n",
    "human_datamatrix = human_datamatrix[[i for i in human_datamatrix.columns if not i.endswith('.1')]]\n",
    "\n",
    "human_datamatrix = human_datamatrix.T # Transpose the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a571e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current sample labels. Sample will consist of 'Sample Type:No Recurrence' and 'Sample Type:Recurrence'\n",
    "hd_labels = list(human_datamatrix['Factors'])\n",
    "set(hd_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6bda40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many 'No Recurrence' samples in the dataset\n",
    "hd_labels.count('Sample Type:No Recurrence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1babbb25",
   "metadata": {},
   "source": [
    "### Blank Treatment\n",
    "\n",
    "- If blank_treatment = True:\n",
    "\n",
    "An average of the blanks will be subtracted to the remaining dataset. Missing values in the blanks were replaced by 0 to calculate the average of the blanks. Negative values that arise in the dataset from subtracting the blanks will be coded as 0/missing values in the dataset.\n",
    "\n",
    "- If blank_treatment = False:\n",
    "\n",
    "Blank samples are removed and not accounted for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_treatment = True\n",
    "#blank_treatment = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2946fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if blank_treatment:\n",
    "    blanks = human_datamatrix[human_datamatrix['Factors'] == 'Sample Type:Blank'].iloc[:,1:] # Select blank samples\n",
    "    blanks = blanks.replace({np.nan:0}) \n",
    "    blanks = blanks.astype(float) # Get blank samples with floats (needed since datamatrix has strings)\n",
    "    blanks_average = blanks.mean() # Average of the blanks\n",
    "    blanks_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58569e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the samples belonging to either the 'No Recurrence' or 'Recurrence' \n",
    "selection = []\n",
    "for i in human_datamatrix.loc[:, 'Factors']:\n",
    "    if i in ['Sample Type:No Recurrence', 'Sample Type:Recurrence']:\n",
    "        selection.append(True)\n",
    "    else:\n",
    "        selection.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef0ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_datamatrix = human_datamatrix[selection]\n",
    "# Creating the list of 'targets' (labels of samples) of the dataset with the 'No Recurrence' and 'Recurrence' classes \n",
    "hd_labels = []\n",
    "for i in list(human_datamatrix.iloc[:,0]):\n",
    "    if i == 'Sample Type:No Recurrence':\n",
    "        hd_labels.append('No Recurrence')\n",
    "    else:\n",
    "        hd_labels.append('Recurrence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4d2c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_datamatrix = human_datamatrix.iloc[:,1:]\n",
    "human_datamatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65097771",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_datamatrix = human_datamatrix.astype(float) # Passing the values from strings to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a65206",
   "metadata": {},
   "outputs": [],
   "source": [
    "if blank_treatment:\n",
    "    human_datamatrix = human_datamatrix.replace({np.nan:0}) - blanks_average\n",
    "    human_datamatrix[human_datamatrix<0] = 0\n",
    "    human_datamatrix = human_datamatrix.replace({0:np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb1de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#human_datamatrix.columns = list(mz_list)[1:]\n",
    "#human_datamatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61528e66",
   "metadata": {},
   "source": [
    "Set list of masses to pass to Cytoscape and to build the MDiNs.\n",
    "\n",
    "'Neutralization' of the _m/z_ values by subtracting the mass of a proton to the _m/z_ peaks.\n",
    "\n",
    "4 peaks (2 pairs) with different retention time have the exact same _m/z_ values. These 2 pairs of peaks wil be treated as only 2 different peaks (instead of four) for MDiN building purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be837647",
   "metadata": {},
   "outputs": [],
   "source": [
    "mz_list = mz_list[1:] - chemdict['H'][0] + electron_mass\n",
    "counts = human_datamatrix.count(axis=0)\n",
    "final_mz_list = list(mz_list[list(counts >= 2)])\n",
    "final_mz_list = set(final_mz_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_datamatrix = transf.keep_atleast(human_datamatrix, minimum=2) # Keep features that appear in at least two samples\n",
    "human_datamatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b53d9",
   "metadata": {},
   "source": [
    "### Building datasets database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e29f0",
   "metadata": {},
   "source": [
    "Atomic mass of the most common naturally occurring isotope of the more important elements in metabolites and their isotopic abundances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a314a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "# From the alignments_new store\n",
    "\n",
    "# GD_neg_global2 (GDg2-)\n",
    "data_df = pd.HDFStore('GD_data.h5').get('all_1ppm_min2_neg').transpose()\n",
    "gd_labels = mtl.parse_data(data_df, labels_loc='label').sample_labels\n",
    "new_cols_df = data_df.columns + chemdict['H'][0] - electron_mass\n",
    "data_df.columns = new_cols_df\n",
    "\n",
    "datasets['GD_neg_global2'] = {'source': 'grapevine',\n",
    "                              'alignment': '1-2',\n",
    "                              'mode': '-',\n",
    "                              'name': 'GDg2-',\n",
    "                              'data': data_df,#.to_dict(orient='split'),\n",
    "                              'original': data_df,#.to_dict(orient='split'),\n",
    "                              'target': gd_labels,\n",
    "                              'classes': list(pd.unique(gd_labels))}\n",
    "\n",
    "# GD_neg_class2 (GDc2-)\n",
    "data_df = pd.HDFStore('GD_data.h5').get('groups_1ppm_min2_all_1ppm_neg').transpose()\n",
    "labels = mtl.parse_data(data_df, labels_loc='label').sample_labels\n",
    "new_cols_df = data_df.columns + chemdict['H'][0] - electron_mass\n",
    "data_df.columns = new_cols_df\n",
    "\n",
    "datasets['GD_neg_class2'] = {'source': 'grapevine',\n",
    "                              'alignment': '2-1',\n",
    "                              'mode': '-',\n",
    "                              'name': 'GDc2-',\n",
    "                              'data': data_df,#.to_dict(orient='split'),\n",
    "                              'original': data_df,#.to_dict(orient='split'),\n",
    "                              'target': gd_labels,\n",
    "                              'classes': list(pd.unique(gd_labels))}\n",
    "\n",
    "# YD (YD)\n",
    "yeast_labels = [item for item in yeast_classes for i in range(3)]\n",
    "\n",
    "datasets['YD'] = {'source': 'yeast',\n",
    "                            'alignment': '1-2',\n",
    "                            'mode': '+',\n",
    "                            'name': 'YD',\n",
    "                            'data': yeast_datamatrix,#.to_dict(orient='split'),\n",
    "                            'original': yeast_datamatrix,#.to_dict(orient='split'),\n",
    "                            'target': yeast_labels,\n",
    "                            'classes': list(pd.unique(yeast_labels))}\n",
    "\n",
    "# vitis_types (GD types)\n",
    "vitis_types = {'CAN': 'wild', 'RIP': 'wild', 'ROT': 'wild','RU': 'wild', 'LAB': 'wild',\n",
    "               'SYL': 'wild','REG': 'vinifera','CS': 'vinifera','PN': 'vinifera','RL': 'vinifera',\n",
    "               'TRI': 'vinifera'}\n",
    "\n",
    "gd_type_labels = [vitis_types[lbl] for lbl in gd_labels]\n",
    "\n",
    "datasets['vitis_types'] = {'source': 'grapevine',\n",
    "                            'alignment': '2-1',\n",
    "                            'mode': '-',\n",
    "                            'name': 'GD types',\n",
    "                            'data': datasets['GD_neg_class2']['original'].copy(),\n",
    "                            'original': datasets['GD_neg_class2']['original'].copy(),\n",
    "                            'target': gd_type_labels,\n",
    "                            'classes': list(pd.unique(gd_type_labels))}\n",
    "\n",
    "# HD (HD)\n",
    "datasets['HD'] = {'source': 'human',\n",
    "                  'alignment': '1-2',\n",
    "                  'mode': '+',\n",
    "                  'name': 'HD',\n",
    "                  'data': human_datamatrix,#.to_dict(orient='split'),\n",
    "                  'original': human_datamatrix,#.to_dict(orient='split'),\n",
    "                  'target': hd_labels,\n",
    "                  'classes': list(pd.unique(hd_labels))}\n",
    "\n",
    "\n",
    "print('target for grapevine 11-variety data sets')\n",
    "print(datasets['GD_neg_global2']['target'])\n",
    "print('------\\ntarget for 4 yeast strains data set')\n",
    "print(datasets['YD']['target'])\n",
    "print('------\\ntarget for 2-class wild Vitis vs Vitis vinifera data set')\n",
    "print(datasets['vitis_types']['target'])\n",
    "print('------\\ntarget for human 2-class dataset')\n",
    "print(datasets['HD']['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162669e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df = pd.HDFStore('alignments_new.h5').get('groups_1ppm_min2_all_1ppm_neg').transpose()\n",
    "#labels = mtl.parse_data(data_df, labels_loc='label').sample_labels\n",
    "#new_cols_df = data_df.columns + chemdict['H'][0] - electron_mass\n",
    "#data_df.columns = new_cols_df\n",
    "\n",
    "#df = datasets['GD_neg_global2']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should not raise AssertionError:\n",
    "#assert_frame_equal(df, data_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c53c20",
   "metadata": {},
   "source": [
    "### Writing a .csv file for each of the datasets with the list of masses that will be used to build the MDiNs (in Cytoscape's MetaNetter)\n",
    "\n",
    "The file needs to be correctly read by Cytoscape's MetaNetter (and Cytoscape) where the network will be built.\n",
    "\n",
    "For this, files with two columns both with the same list of masses (one index, one in the column) will be made. One will correspond to the network nodes names as a 'string'. The other will be an attribute of the corresponding node as a 'float' and will be used in the building process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac82682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dskey, ds in datasets.items():\n",
    "        \n",
    "    if dskey.startswith('HD'):  \n",
    "        # Specific mass lists for HD \n",
    "        # (two pairs of masses with same m/z values and different retention times are only treated as 2 different m/z)\n",
    "        final_mz_list = set(final_mz_list)\n",
    "        pd.DataFrame(final_mz_list, index=final_mz_list).to_csv('mass_data/MassList' + dskey + '.csv')\n",
    "        continue\n",
    "    \n",
    "    # Creating mass lists in .csv files for MDiN building\n",
    "    pd.DataFrame(ds['data'].columns, index=ds['data'].columns).to_csv('mass_data/MassList' + dskey + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da21ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d68ee29",
   "metadata": {},
   "source": [
    "### Dataset Characteristics\n",
    "\n",
    "Building a table with general characteristics about the datasets studied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb0c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_dataset(dskey, ds):\n",
    "    \"Computes and returns some general characteristics of a dataset in a dictionary.\"\n",
    "\n",
    "    dataset_chrs = {}\n",
    "    \n",
    "    name = ds['name'] # Name of the dataset\n",
    "    n_samples, n_feats = ds['data'].shape\n",
    "    n_classes = len(ds['classes'])\n",
    "       \n",
    "    Feat_Sample = ds['data'].count(axis=1) # Nº Features in each sample\n",
    "    Min_Feat_Sample = str(Feat_Sample.min()) # Minimum nº Features in a sample\n",
    "    Max_Feat_Sample = str(Feat_Sample.max()) # Maximum nº Features in a sample\n",
    "    Average_Feat_Sample = Feat_Sample.mean() # Average nº Features in a sample\n",
    "    \n",
    "    avg_feat_per_sample = int(round(Average_Feat_Sample,0)) # Round\n",
    "    \n",
    "    Samp_Class = len(ds['target'])/len(ds['classes']) # Nº Sample per Class\n",
    "    if dskey == 'vitis_types':\n",
    "        Samp_Class = '15 Vitis vinifera, 18 Wild'\n",
    "        #Samp_Class = '15 $\\it{Vitis}$ $\\it{Vinifera}$, 18 Wild'\n",
    "    elif dskey == 'HD':\n",
    "        Samp_Class = '114 Recurrence, 135 no Recurrence'\n",
    "    else:\n",
    "        Samp_Class = str(int(Samp_Class))\n",
    "    \n",
    "    n_na = ds['data'].isna().sum().sum() # Nº of missing values in the dataset\n",
    "    \n",
    "    p_na = round(100.0 * n_na / (n_samples * n_feats), 2) # % of missing values in the dataset\n",
    "    \n",
    "    avg_na_per_feature = (n_samples - ds['data'].count(axis=0)).mean()\n",
    "    avg_na_per_feature = int(round(avg_na_per_feature, 0))\n",
    "    \n",
    "    return {'Data set': name,\n",
    "            '# samples': n_samples,\n",
    "            '# features': n_feats,\n",
    "            'features / sample (range)': f'{avg_feat_per_sample} ({Min_Feat_Sample}-{Max_Feat_Sample})',\n",
    "            '# classes': n_classes,\n",
    "            'samples / class':Samp_Class,\n",
    "            '% missing values': p_na,} \n",
    "            #'missing values / feature': avg_na_per_feature}\n",
    "\n",
    "data_characteristics = [characterize_dataset(dskey, ds) for dskey, ds in datasets.items()]\n",
    "data_characteristics = pd.DataFrame(data_characteristics).set_index('Data set')\n",
    "data_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb78a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_characteristics.to_excel('dataset_characteristics.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f5e1f",
   "metadata": {},
   "source": [
    "## Sample Mass-Difference Network Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce2dbcf",
   "metadata": {},
   "source": [
    "#### MDBs (Mass-Difference-based Building blocks) \n",
    "\n",
    "Building the **list of MDBs** to use when building the MDiNs.\n",
    "\n",
    "MDB - Mass-Difference-based Building blocks.\n",
    "\n",
    "The choice of this MDBs is then crucial in the network building process. Ideally, they should represent different 'simple' biochemical reactions that cover the most common and ubiquitous enzymatic and non-enzymatic reactions while also being a relatively small amount of reactions – a total of 15 were picked - and maintaining the metabolite formula charge neutrality. \n",
    "\n",
    "For example, to maintain neutrality, a phosphorylation would mean the overall addition of a PO3H – addition of a -PO3$^{2-}$ group + 2 H$^+$ (maintaining neutrality) to replace an H atom in a metabolite. All the MDBs chosen represent changes in metabolites of no more than 5 atoms and less than 80 Da (small size). Each MDB should represent a set of chemically known reactions and a change in every main element in metabolites (C, H, O, N, S and P) is represented by at least one of the MDBs. To fulfil these conditions, representative MDBs were searched using BRENDA (https://www.brenda-enzymes.org/). The groups chosen were the following:\n",
    "\n",
    "- CH2 (methylation) \n",
    "- O (oxygenation) \n",
    "- H2 (Hydrogenation)\n",
    "- O(N-H-) (Aminase): NH3(O-) - H2\n",
    "- PO3H (phosphorylation)\n",
    "- NH3(O-) (transaminases)\n",
    "- SO3 (sulphation)\n",
    "- CO (like formylation) \n",
    "- CO2 (carboxylation, decarboxylation)\n",
    "- CHOH (Hydroxymethylation) \n",
    "- NCH (formidoyltransferase)\n",
    "- CONH (carbamoyltransferase)\n",
    "- C2H2O (acetylation)\n",
    "- S (rare but an extra S reaction)\n",
    "- H2O\n",
    "\n",
    "For the YD dataset, an extra 2 MDBs were used as an example of specific MDBs that can be used for certain datasets. These two MDBs were added since these reactions are known to occur with higher glycation in a biological system. Despite mainly targeting proteins, some polyssaccarides can also be glycated. Since the yeast mutants gene deletion affect enzymes involved in methylglyoxal metabolism (ΔGLO1, ΔGLO2, ΔGRE3), which is an anti-glycation mechanism in the cell.\n",
    "\n",
    "- CHCOOH (carboxymethylation)\n",
    "- CCH3COOH (carboxyethylation)\n",
    "\n",
    "There could be many other MDB representing other reactions that can also be included such as CN2H2 (amidinotransferases), COCH2COO (malonyl transferases), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d02e752",
   "metadata": {},
   "source": [
    "Functions to transform formulas in string format to dictionary format and from calculate formulas (in dictionary format) exact masses.\n",
    "\n",
    "Example:\n",
    "\n",
    "    'String' format: 'C6H1206'\n",
    "    \n",
    "    'Dictionary' format: {'C':6, 'H':12, 'O':6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bcc410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formula_process(formula):\n",
    "    \"\"\"Transforms a formula in string format into a dictionary.\"\"\"\n",
    "\n",
    "    # Empty dictionary to store the results\n",
    "    results = dict.fromkeys(['C','H','O','N','S','P','Cl','F'], 0)\n",
    "    count = ''\n",
    "    letter = None\n",
    "    minus = False\n",
    "    \n",
    "    # Run through the string\n",
    "    for i in range(len(formula)):\n",
    "        if formula[i].isupper(): # If i is an uppercase letter then it is an element\n",
    "            if letter: # Just to account for the first letter in the formula where letter is None \n",
    "                # Reached another letter, store previous results and reset count\n",
    "                if minus:\n",
    "                    results[letter] = - int(count or 1)\n",
    "                    minus = False\n",
    "                else:\n",
    "                    results[letter] = int(count or 1)\n",
    "                count = ''\n",
    "                \n",
    "            if i+1 < len(formula): # In case it's a two letter element such as Cl\n",
    "                if formula[i+1].islower(): # The second letter is always lower case\n",
    "                    letter = formula[i] + formula[i+1] # Store new 2 letter element\n",
    "                    continue\n",
    "                    \n",
    "            letter = formula[i] # Store new 1 letter element\n",
    "            \n",
    "        elif formula[i].isdigit():\n",
    "            count = count + formula[i] # If number, add number to count\n",
    "        \n",
    "        elif formula[i] == '-':\n",
    "            minus = True\n",
    "    \n",
    "    # Store results of the last letter\n",
    "    if minus:\n",
    "        results[letter] = - int(count or 1)\n",
    "        minus = False\n",
    "    else:\n",
    "        results[letter] = int(count or 1)\n",
    "                    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1077c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmass(c,h,o,n,s,p,cl,f):\n",
    "    \"Get the exact monoisotopic mass for any formula with the C,H,O,N,S,P,Cl and F elements only.\"\n",
    "    massC = chemdict['C'][0] * c\n",
    "    massH = chemdict['H'][0] * h\n",
    "    massO = chemdict['O'][0] * o\n",
    "    massN = chemdict['N'][0] * n\n",
    "    massS = chemdict['S'][0] * s\n",
    "    massP = chemdict['P'][0] * p\n",
    "    massCl = chemdict['Cl'][0] * cl\n",
    "    massF = chemdict['F'][0] * f \n",
    "\n",
    "    massTotal = massC + massH + massO + massN + massS + massP + massCl + massF\n",
    "\n",
    "    return massTotal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9351a69c",
   "metadata": {},
   "source": [
    "#### Making the transformation file with MDBs to use in Cytoscape\n",
    "\n",
    "The transformation file will be in a format accepted by Cytoscape's MetaNetter (.txt separated by tabs) to be considered as a list of transformations (with specific masses) to make the MDiN. 4 columns: 'Label', 'Formula', 'Mass', 'Selected' with mass being the most important.\n",
    "\n",
    "Formula changes will be put under the 'Label' column and the corresponding set of reactions they represent will be put under 'Formula'. This switch is to make the MDB Impact analysis done later down the line more legible (since 'formula changes' are easier to observe, quickly understood and more concise in relation to long-winded names for set of reactions).\n",
    "\n",
    "The 'Selected' column will be just a set of True for the MDBs to come in pre-selected in Cytoscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce51c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataFrame\n",
    "trans_groups = pd.DataFrame(columns=['Label','Formula','Mass','Selected'])\n",
    "trans_groups = trans_groups.set_index('Label')\n",
    "\n",
    "# Chemical Formula transformations (MDBs chosen)\n",
    "MDBs = ['H2','CH2','CO2','O','CHOH','NCH','O(N-H-)','S','CONH','PO3H','NH3(O-)','SO3','CO', 'C2H2O', 'H2O']\n",
    "# Examples of the set of Reactions each MDB represents\n",
    "labels = ['Hydrogenation','Methylation','Carboxylation/Decarboxylation','Oxygenation','Hydroxymethylation',\n",
    "          'Formidoylation','Deamination','Tranfer of and -SH group','Carbamoylation','Phosphorylation','Transamination',\n",
    "          'Sulphation','Formylation','Acetylation','Condensation/Dehydration/Cyclization']\n",
    "\n",
    "for i in range(len(MDBs)): # Passing through each transformation/MDB\n",
    "\n",
    "    # Calculate the exact mass of the MDBs\n",
    "    form = formula_process(MDBs[i])\n",
    "    MDB_mass = getmass(form['C'],form['H'],form['O'],form['N'],form['S'],form['P'],form['Cl'],form['F'])\n",
    "        \n",
    "    # Making the row for each transformation\n",
    "    string = MDBs[i]\n",
    "    trans_groups.loc[string] = (labels[i], MDB_mass, 'true')\n",
    "\n",
    "trans_groups = trans_groups.sort_values(by='Mass')\n",
    "trans_groups # Transformations DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the df as a .txt file in the described format\n",
    "trans_groups.to_csv('transgroups.txt', header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98541921",
   "metadata": {},
   "source": [
    "MDB Transformation file for the YD dataset (that includes the aforementioned two extra MDBs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e610142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataFrame\n",
    "trans_groups = pd.DataFrame(columns=['Label','Formula','Mass','Selected'])\n",
    "trans_groups = trans_groups.set_index('Label')\n",
    "\n",
    "# Chemical Formula transformations (MDBs chosen)\n",
    "MDBs = ['H2','CH2','CO2','O','CHOH','NCH','O(N-H-)','S','CONH','PO3H','NH3(O-)','SO3','CO', 'C2H2O', 'H2O', \n",
    "        'C2H2O2', 'C3H4O2']\n",
    "# Examples of the set of Reactions each MDB represents\n",
    "labels = ['Hydrogenation','Methylation','Carboxylation/Decarboxylation','Oxygenation','Hydroxymethylation',\n",
    "          'Formidoylation','Deamination','Tranfer of and -SH group','Carbamoylation','Phosphorylation','Transamination',\n",
    "          'Sulphation','Formylation','Acetylation','Condensation/Dehydration/Cyclization', 'Carboxymethylation',\n",
    "         'Carboxyehtylation']\n",
    "\n",
    "for i in range(len(MDBs)): # Passing through each transformation/MDB\n",
    "\n",
    "    # Calculate the exact mass of the MDBs\n",
    "    form = formula_process(MDBs[i])\n",
    "    MDB_mass = getmass(form['C'],form['H'],form['O'],form['N'],form['S'],form['P'],form['Cl'],form['F'])\n",
    "        \n",
    "    # Making the row for each transformation\n",
    "    string = MDBs[i]\n",
    "    trans_groups.loc[string] = (labels[i], MDB_mass, 'true')\n",
    "\n",
    "trans_groups = trans_groups.sort_values(by='Mass')\n",
    "trans_groups # Transformations DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6211b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the df as a .txt file in the described format\n",
    "trans_groups.to_csv('transgroups_YD.txt', header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f46bbc2",
   "metadata": {},
   "source": [
    "### Build the Mass-Difference Networks in Cytoscape using MetaNetter 2.0\n",
    "\n",
    "To build the network in Cytoscape we'll need:\n",
    "\n",
    "1) A list of neutral masses (with masses as float in node attributes).\n",
    "\n",
    "2) A list of allowed transformations with specific mass differences - list of MDBs that we build above.\n",
    "\n",
    "The transformation files and the dataset files were used in Cytoscape to build the MDiNs.\n",
    "\n",
    "Parameters: 1 ppm error allowed for edge establishment.\n",
    "\n",
    "The Yeast datasets already have the m/z 'buckets' that are already representing the neutral masses of the metabolites. To consider the neutral masses of the grapevine datasets, a simple transformation by adding or removing a proton (Hydrogen atom mass - electron mass), depending if the ionization mode is negative or positive, respectively, on the m/z peaks was made.\n",
    "\n",
    "\n",
    "Only one full network (for each benchmark dataset) is built. Then, subgraphs of them will be used to select every sample MDiN. This is the same as building an MDiN for each sample.\n",
    "\n",
    "The networks were exported in graphml format that networkX module can read.\n",
    "\n",
    "- Nodes have a standard number ID instead of the mass (stored as the attribute 'mass'). Other attributes stored are irrelevant. \n",
    "- Edges among the different attributes have a very useful attribute called 'Transformation' which stores which MDB of the list was used to establish the edge - will be used for MDB Impact analysis and Weighted MDB Impact.\n",
    "- Finally, the graph is directed. Since reactions are bidireccional, they will be transformed to undirected graphs.\n",
    "\n",
    "Changes that will be made to the network:\n",
    "\n",
    "- Nodes will be identified by their masses.\n",
    "- Intensities of the node in each sample will be given in other notebooks to store for each specific subgraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696665d8",
   "metadata": {},
   "source": [
    "## Data transformations and pre-treatments\n",
    "\n",
    "Each dataset is transformed by 2 combinations of missing value imputation and data pre-treatment methods (generating datasets with different treatments applied).\n",
    "\n",
    "### Traditional and more established  intensity-based Pre-Treatments\n",
    "\n",
    "Missing value imputation is mandatory for the datasets since many statistical methods performed downstream can't handle missing values. \n",
    "\n",
    "Two missing value imputation procedures were applied before any intensity-based pre-treatments: \n",
    "\n",
    "- Missing values of a sample were replaced with a fifth of the minimum intensity value present in that sample of the data matrix - Limit of Detection type of imputation. This was performed with the `fillna_frac_sample_min` function below. \n",
    "\n",
    "- Missing values were imputed by Random Forest missing value imputation. Parameters: 10 trees, 100 nearest features considered, 0 minimum value, 1.0e10 maximum value and 5 max iterations for all datasets except HD (2 max iterations), other parameters were left as default. This was performed by using the scikit-learn Python module with `IterativeImputer`and `ExtraTreesRegressor` as estimator. In cases where this method was used, an extra **`_RF`** was added to the 'name' of the pre-treatment.\n",
    "\n",
    "#### Combinations of intensity-based pre-treatments made:\n",
    "\n",
    "- Ionly Treatment - Only Missing Value Imputation.\n",
    "\n",
    "- **NGP Treatment** - Missing Value Imputation, Normalization, Generalized Logarithmic Transformation and Pareto Scaling.\n",
    "\n",
    "For normalizations, Normalization by reference feature (Leucine Enkephalin) is used in all cases, except for the HD dataset where Probabilistic Quotient Normalization (PQN) is used with the mean of all samples acting as the reference sample.\n",
    "\n",
    "Note: Leucine Enkephalin peak (reference feature) is removed upon normalization. Order of pre-treatments is the order in which they were mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7036d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_RF(df, nearest_features=100, n_trees=50, max_iter=10):\n",
    "    \"Random Forest Imputation of Missing Values.\"\n",
    "    rf_estimator = ExtraTreesRegressor(n_estimators=n_trees)\n",
    "    imputer = IterativeImputer(random_state=0, estimator=rf_estimator,\n",
    "                           n_nearest_features=nearest_features,\n",
    "                           min_value=0.0, max_value=1.0e10,\n",
    "                           max_iter=max_iter,\n",
    "                           verbose=0)\n",
    "    imputed_data = imputer.fit_transform(df)\n",
    "    res = pd.DataFrame(imputed_data, index=df.index, columns=df.columns)\n",
    "    ncols = len(res.columns)\n",
    "    res = res.dropna(axis='columns', how='any')\n",
    "    ncols2 = len(res.columns)\n",
    "    if ncols > ncols2:\n",
    "        print(f'{ncols-ncols2} features dropped')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630bb8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For missing value imputation based on constants relative to the minimum of each feature instead of the full dataset\n",
    "def fillna_frac_feat_min(df, fraction=0.2):\n",
    "    \"\"\"Set NaN to a fraction of the minimum value in each column of the DataFrame.\"\"\"\n",
    "\n",
    "    minimum = df.min(axis=0) * fraction\n",
    "    return df.fillna(minimum)\n",
    "\n",
    "# For missing value imputation based on constants relative to the minimum of each sample instead of the full dataset\n",
    "def fillna_frac_sample_min(df, fraction=0.2):\n",
    "    \"\"\"Set NaN to a fraction of the minimum value in each row of the DataFrame.\"\"\"\n",
    "\n",
    "    minimum = df.min(axis=1) * fraction\n",
    "    return df.T.fillna(minimum).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9748e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs all pre-treatment combinations mentioned\n",
    "def compute_transf(dataset, norm_ref=None, lamb=None, max_iter=10):\n",
    "    \"Computes pre-treatments and missing value imputations and returns after treatment datasets in a dict.\"\n",
    "    \n",
    "    data = dataset['data']\n",
    "    \n",
    "    # Imputation of Missing Values\n",
    "    #imputed = fillna_frac_feat_min(data, fraction=0.2)\n",
    "    imputed = fillna_frac_sample_min(data, fraction=0.2)\n",
    "    \n",
    "    # Imputation by RF\n",
    "    datacols = data.columns\n",
    "    data.columns = [str(col) for col in data.columns]\n",
    "    imputedRF = impute_RF(data, nearest_features=100, n_trees=10, max_iter=max_iter)\n",
    "    imputedRF.columns = datacols\n",
    "    imputed.columns = datacols\n",
    "    \n",
    "    # Normalization by a reference feature\n",
    "    if norm_ref is not None:\n",
    "        norm = transf.normalize_ref_feature(imputed, norm_ref, remove=True)\n",
    "    else:\n",
    "        norm = transf.normalize_PQN(imputed, ref_sample='mean')\n",
    "    \n",
    "    # Normalization by a reference feature RF\n",
    "    if norm_ref is not None:\n",
    "        normRF = transf.normalize_ref_feature(imputedRF, norm_ref, remove=True)\n",
    "    else:\n",
    "        normRF = transf.normalize_PQN(imputedRF, ref_sample='mean')\n",
    "    \n",
    "    # Normalization by PQN for HD - 1/2 and RF   \n",
    "    #if dataset['name'].startswith('HD'):\n",
    "      #  norm = transf.normalize_PQN(imputed, ref_sample='mean')\n",
    "      #  normRF = transf.normalize_PQN(imputedRF, ref_sample='mean')\n",
    "\n",
    "    # Pareto Scaling and Generalized Logarithmic Transformation\n",
    "    NGP = transf.pareto_scale(transf.glog(norm, lamb=lamb))\n",
    "\n",
    "    # Pareto Scaling and Generalized Logarithmic Transformation\n",
    "    NGP_RF = transf.pareto_scale(transf.glog(normRF, lamb=lamb))\n",
    "    \n",
    "    # Store results\n",
    "    dataset['Ionly'] = imputed\n",
    "    dataset['NGP'] = NGP\n",
    "    \n",
    "    dataset['Ionly_RF'] = imputedRF\n",
    "    dataset['NGP_RF'] = NGP_RF    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d636d16e",
   "metadata": {},
   "source": [
    "Human leucine enkephalin (Sigma Aldrich) is the reference feature (internal standard, [M+H]+ = 556.276575 Da or [M-H]- = 554.262022 Da) used for these datasets.\n",
    "\n",
    "Search in the grapevine data sets for the reference feature and confirm the reference feature in the yeast data sets with `find_closest_features` from metabolinks package.\n",
    "\n",
    "There is **no** reference feature in the HD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f59d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical mass for negative mode Leucine Enkephalin - 554.262022\n",
    "# Theoretical mass for positive mode Leucine Enkephalin - 556.276575\n",
    "Leu_Enk_neg = 554.262022 + chemdict['H'][0] - electron_mass\n",
    "Leu_Enk_pos = 556.276575 - chemdict['H'][0] + electron_mass\n",
    "\n",
    "# Reference Feature in the yeast dataset\n",
    "leu_enk_name = 555.2692975341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a3edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_datasets = [name for name in datasets if datasets[name]['source']=='grapevine']\n",
    "\n",
    "for name in query_datasets:\n",
    "    ds = datasets[name]\n",
    "    print(f'looking for reference in {name} ...')\n",
    "    ref_variable = Leu_Enk_neg if ds['mode'] == '-' else Leu_Enk_pos\n",
    "    closest = transf.find_closest_features(ds['data'], features=[ref_variable])\n",
    "    if closest[ref_variable] is not None:\n",
    "        print('Found ref feature', ref_variable)\n",
    "        delta = closest[ref_variable] - ref_variable\n",
    "        print(f'In data: {closest[ref_variable]} delta = {delta:.3e}\\n')\n",
    "\n",
    "query_datasets = [name for name in datasets if name.startswith('YD')]\n",
    "\n",
    "ref_variable = leu_enk_name\n",
    "for name in query_datasets:\n",
    "    ds = datasets[name]\n",
    "    print(f'looking for reference in {name} ...') \n",
    "    closest = transf.find_closest_features(ds['data'], features=[ref_variable])\n",
    "    if closest[ref_variable] is not None:\n",
    "        print('Found ref feature', ref_variable, '\\n')\n",
    "    else:\n",
    "        print('Ref feature not found\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b590441d",
   "metadata": {},
   "source": [
    "##### Apply the different pre-treatments and get the results in their respective dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0908696",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for name, ds in datasets.items():\n",
    "    dataset_name = datasets[name][\"name\"]\n",
    "    print(f'Applying pre-processing transformations to data in {dataset_name}', end=' ...')\n",
    "    start = perf_counter()\n",
    "    \n",
    "    max_iter = 5\n",
    "    \n",
    "    if name.startswith('YD'):\n",
    "        ref_variable = leu_enk_name\n",
    "    elif name.startswith('HD'):\n",
    "        ref_variable = None\n",
    "        max_iter = 2\n",
    "    else:\n",
    "        ref_variable = Leu_Enk_neg if ds['mode'] == '-' else Leu_Enk_pos\n",
    "    \n",
    "    compute_transf(ds, norm_ref=ref_variable, max_iter=max_iter)\n",
    "    \n",
    "    end = perf_counter()\n",
    "\n",
    "    print(f'done! took {(end - start):.3f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea632641",
   "metadata": {},
   "source": [
    "### Splitting data into test and train groups and performing independent data pre-treatments\n",
    "\n",
    "For supervised analysis, where stratified k-fold cross-validation is used to assess model performance over 20 iterations (to consider different combinations of fold splits), the separation of the datasets into the train and testing folds for each iteration is generated.\n",
    "\n",
    "This is generated now so we can perform the previously mentioned data pre-treatment independently to the training and testing set for each iteration and each 'fold' in order to avoid the data leakage that may happen to the established intensity based data pre-treatment.\n",
    "\n",
    "For the features in the training sets that do not appear in test sets; these are added to the test data with the lowest value in the training set for that feature being assigned to all samples of the test set.\n",
    "\n",
    "For the set of pre-treatments that use Random Forest Imputation, since you need at least 2 values in each feature to perform imputation, training sets were filtered so as to each features is at least present in 2 samples. In test sets, RF Imputation is performed on features that are present in at least 2 samples, but since our data has a very limited number of samples and each test data only has 1 sample of each class, we cannot eliminate features that are only present in one of the samples of the test group since that feature might be characteristic and act as a biomarker of the class that sample belongs to. To counteract that, 1/5 min imputation (a fifth of the minimum intensity value present in that sample of the data matrix) is performed on those features specifically to keep their potential information in the test data. Only features that also appear in the training data are kept for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e4a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5001d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altered version of compute_transf to take into account training and testing sets\n",
    "def compute_transf_iterations(dataset, norm_ref=None, lamb=None, train=True, max_iter=10):\n",
    "    \"Computes combinations of pre-treatments and returns after treatment datasets in a dict.\"\n",
    "    \n",
    "    data = dataset['data'].copy()\n",
    "    data = data.dropna(how='all', axis=1)\n",
    "    \n",
    "    # Imputation of Missing Values\n",
    "    imputed = fillna_frac_sample_min(data, fraction=0.2)\n",
    "    \n",
    "    # Imputation by RF\n",
    "    # If training set, keep only features that appear at least in 2 samples\n",
    "    if train:\n",
    "        data_temp = transf.keep_atleast(data, minimum=2)\n",
    "    # If test set, separate data between features that appear in at least 2 samples and those that only appear in 1.\n",
    "    # Treat the data that only appears in 1 sample with with 1/5 min (of the sample) imputation.\n",
    "    # Treat the rest of the data normally with RF Imputation as with the training set.\n",
    "    # Then re-join the two sets of data\n",
    "    else:\n",
    "        data_temp = transf.keep_atleast(data, minimum=2).copy()\n",
    "        remaining_data = data[np.setdiff1d(data.columns,data_temp.columns)]\n",
    "        \n",
    "        # Replacing missing values in remaining data with 1/5 of the minimum value in each sample\n",
    "        fraction = 0.2\n",
    "        minimum = data.min(axis=1) * fraction # Minimum value in each sample\n",
    "        remaining_data = remaining_data.T.fillna(minimum).T\n",
    "\n",
    "    datacols = data_temp.columns\n",
    "    data_temp.columns = [str(col) for col in data_temp.columns]\n",
    "    #print(data)\n",
    "    imputedRF = impute_RF(data_temp, nearest_features=100, n_trees=10, max_iter=max_iter)\n",
    "    imputedRF.columns = datacols\n",
    "    #imputed.columns = datacols\n",
    "    \n",
    "    # Join the split data\n",
    "    if not train:\n",
    "        imputedRF = pd.concat((imputedRF, remaining_data), axis=1)\n",
    "    \n",
    "    # Normalization by a reference feature\n",
    "    if norm_ref is not None:\n",
    "        norm = transf.normalize_ref_feature(imputed, norm_ref, remove=True)\n",
    "    else:\n",
    "        norm = transf.normalize_PQN(imputed, ref_sample='mean')\n",
    "    \n",
    "    # Normalization by a reference feature RF\n",
    "    if norm_ref is not None:\n",
    "        normRF = transf.normalize_ref_feature(imputedRF, norm_ref, remove=True)\n",
    "    else:\n",
    "        normRF = transf.normalize_PQN(imputedRF, ref_sample='mean')\n",
    "    \n",
    "    # Pareto Scaling and Generalized Logarithmic Transformation\n",
    "    NGP = transf.pareto_scale(transf.glog(norm, lamb=lamb))\n",
    "\n",
    "    # Pareto Scaling and Generalized Logarithmic Transformation\n",
    "    NGP_RF = transf.pareto_scale(transf.glog(normRF, lamb=lamb))\n",
    "    \n",
    "    # Store results\n",
    "    dataset['Ionly'] = imputed\n",
    "    dataset['NGP'] = NGP\n",
    "    \n",
    "    dataset['Ionly_RF'] = imputedRF\n",
    "    dataset['NGP_RF'] = NGP_RF    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee7efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations that will be used for cross-validation\n",
    "iter_num = 20\n",
    "\n",
    "for name, ds in datasets.items():\n",
    "    \n",
    "    # Save the HD for later\n",
    "    if name == 'HD':\n",
    "        continue\n",
    "    \n",
    "    np.random.seed(16)\n",
    "    \n",
    "    start2 = perf_counter()\n",
    "    # Set up the place where data will be stored\n",
    "    ds['iter_fold_splits'] = {}\n",
    "    \n",
    "    # Set up ref_variable for normalization and iteration number for imputation by RF\n",
    "    max_iter = 2\n",
    "    \n",
    "    if name.startswith('YD'):\n",
    "        ref_variable = str(leu_enk_name)\n",
    "    elif name.startswith('HD'):\n",
    "        ref_variable = None\n",
    "    else:\n",
    "        ref_variable = '555.269296452291' #if ds['mode'] == '-' else '555.269293547709'\n",
    "    \n",
    "    # In each iteration\n",
    "    for n_iter in range(iter_num):\n",
    "        n_fold = 5 if name in ('vitis_types', 'HD') else 3\n",
    "        kf = StratifiedKFold(n_fold, shuffle=True, random_state=307*(n_iter+1))\n",
    "        \n",
    "        # Set up different storages\n",
    "        ds['iter_fold_splits'][n_iter+1] = {}\n",
    "        test_dict = ds['iter_fold_splits'][n_iter+1]\n",
    "        test_dict['train'] = {} # For training set\n",
    "        test_dict['test'] = {} # For test set\n",
    "        fold = 1\n",
    "        start = perf_counter()\n",
    "        \n",
    "        # Generate each fold of training and test sets for each iteration, treat them and store them\n",
    "        for train_index, test_index in kf.split(ds['data'], ds['target']):\n",
    "            \n",
    "            # Generating\n",
    "            test_dict['train'][fold] = {}\n",
    "            test_dict['test'][fold] = {}\n",
    "            test_dict['train'][fold]['data'], test_dict['test'][fold]['data'] = ds['data'].iloc[\n",
    "                train_index, :], ds['data'].iloc[test_index, :]\n",
    "            test_dict['train'][fold]['target'], test_dict['test'][fold]['target'] = [\n",
    "                ds['target'][i] for i in train_index], [ds['target'][i] for i in test_index]\n",
    "            \n",
    "            # Treating\n",
    "            compute_transf_iterations(test_dict['train'][fold], norm_ref=ref_variable, train=True, max_iter=max_iter)\n",
    "            compute_transf_iterations(test_dict['test'][fold], norm_ref=ref_variable, train=False, max_iter=max_iter)\n",
    "            \n",
    "            # Adding features that do not appear in the test data but appear in the training data to the test data\n",
    "            # with the minimum value of said feature in the training data \n",
    "            train_cols = test_dict['train'][fold]['data'].dropna(how='all', axis=1).columns\n",
    "            test_cols = test_dict['test'][fold]['data'].dropna(how='all', axis=1).columns\n",
    "            cols_to_add = np.setdiff1d(train_cols,test_cols)\n",
    "            \n",
    "            # Add to the test dict those features\n",
    "            for treat in ('Ionly', 'NGP', 'Ionly_RF', 'NGP_RF'):\n",
    "                test_numb = len(test_dict['test'][fold][treat])\n",
    "                df_prep = []\n",
    "                for i in cols_to_add:\n",
    "                    df_prep.append(min(test_dict['train'][fold][treat][i]))\n",
    "\n",
    "                df = test_dict['test'][fold][treat].copy()\n",
    "                df = pd.concat((test_dict['test'][fold][treat], \n",
    "                                pd.DataFrame((df_prep, ) * test_numb, \n",
    "                                             index=df.index, columns=cols_to_add)), axis=1)\n",
    "                test_dict['test'][fold][treat] = df[\n",
    "                    test_dict['train'][fold][treat].columns]\n",
    "\n",
    "            fold = fold+1\n",
    "\n",
    "        #ds['iter_fold_splits'][n_iter+1] = test_dict\n",
    "        end = perf_counter()\n",
    "        print(name, n_iter+1, f' iteration done! took {(end - start):.3f} s')\n",
    "    \n",
    "    end2 = perf_counter()\n",
    "    print(name, f' dataset done! took {(end2 - start2):.3f} s!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e726bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the folds for each iteration of cross-validation and pre-treating for the HD dataset \n",
    "np.random.seed(214)\n",
    "iter_num = 20\n",
    "\n",
    "name, ds = 'HD', datasets['HD']\n",
    "\n",
    "# Split the dataset into an external training and testing group\n",
    "train_data, test_data, train_target, test_target = train_test_split(ds['data'], ds['target'], test_size=0.3)\n",
    "\n",
    "ds['train'] = {}\n",
    "ds['train']['data'] = train_data\n",
    "ds['train']['target'] = train_target\n",
    "\n",
    "ds['test'] = {}\n",
    "ds['test']['data'] = test_data\n",
    "ds['test']['target'] = test_target\n",
    "\n",
    "# Treat each group independently\n",
    "start = perf_counter()\n",
    "compute_transf_iterations(ds['train'], norm_ref=None, train=True, max_iter=2)\n",
    "compute_transf_iterations(ds['test'], norm_ref=None, train=False, max_iter=2)\n",
    "\n",
    "# Drop features that do not appear in the training set and add features that do not appear in the test data but appear\n",
    "# in the training data to the test data with the minimum value of said feature in the training data \n",
    "train_cols = ds['train']['data'].dropna(how='all', axis=1).columns\n",
    "test_cols = ds['test']['data'].dropna(how='all', axis=1).columns\n",
    "cols_to_add = np.setdiff1d(train_cols,test_cols)\n",
    "\n",
    "for treat in ('Ionly', 'NGP', 'Ionly_RF', 'NGP_RF'):\n",
    "    test_numb = len(ds['test'][treat])\n",
    "    df_prep = []\n",
    "    for i in cols_to_add:\n",
    "        df_prep.append(min(ds['train'][treat][i]))\n",
    "\n",
    "    df = ds['test'][treat].copy()\n",
    "    df = pd.concat((ds['test'][treat], pd.DataFrame((df_prep, ) * test_numb, index=df.index, columns=cols_to_add)), axis=1)\n",
    "    ds['test'][treat] = df[ds['train'][treat].columns] # Same order as training set\n",
    "\n",
    "end = perf_counter()\n",
    "print(name, f' main test and training data pre-treatment done! took {(end - start):.3f} s')\n",
    "\n",
    "# Now, from the training set, generate the folds for every cross-validation iteration and pre-treat each of them\n",
    "# with each pre-treatment\n",
    "np.random.seed(16)\n",
    "# Where it will be stored\n",
    "ds['iter_fold_splits'] = {}\n",
    "\n",
    "# Each Iteration\n",
    "for n_iter in range(iter_num):\n",
    "    \n",
    "    # Generate each fold of training and test sets for each iteration, treat them and store them\n",
    "    n_fold = 5\n",
    "    kf = StratifiedKFold(n_fold, shuffle=True, random_state=307*(n_iter+1))\n",
    "    ds['iter_fold_splits'][n_iter+1] = {}\n",
    "    test_dict = ds['iter_fold_splits'][n_iter+1]\n",
    "    test_dict['train'] = {}\n",
    "    test_dict['test'] = {}\n",
    "    fold = 1\n",
    "    start = perf_counter()\n",
    "\n",
    "    # Only use data from the main training set\n",
    "    for train_index, test_index in kf.split(ds['train']['data'], ds['train']['target']):\n",
    "        start2 = perf_counter()\n",
    "        # Generating\n",
    "        test_dict['train'][fold] = {}\n",
    "        test_dict['test'][fold] = {}\n",
    "        test_dict['train'][fold]['data'], test_dict['test'][fold]['data'] = ds['train']['data'].iloc[\n",
    "            train_index, :], ds['train']['data'].iloc[test_index, :]\n",
    "        test_dict['train'][fold]['target'], test_dict['test'][fold]['target'] = [\n",
    "            ds['train']['target'][i] for i in train_index], [ds['train']['target'][i] for i in test_index]\n",
    "\n",
    "        # Treating\n",
    "        compute_transf_iterations(test_dict['train'][fold], norm_ref=None, train=True, max_iter=2)\n",
    "        compute_transf_iterations(test_dict['test'][fold], norm_ref=None, train=False, max_iter=2)\n",
    "\n",
    "        train_cols = test_dict['train'][fold]['data'].dropna(how='all', axis=1).columns\n",
    "        test_cols = test_dict['test'][fold]['data'].dropna(how='all', axis=1).columns\n",
    "        cols_to_add = np.setdiff1d(train_cols,test_cols)\n",
    "\n",
    "        # An extra safe net that needs to be applied due to the earlier split into a static test and training group\n",
    "        # This might lead to features that are present in 2 samples, for example, to have a chance of one being in the\n",
    "        # test group and another in the training. Thus, in those case features had to be removed before RF_Imputation\n",
    "        # (requires a feature to be present in at least two samples), so the set of features in the data from 1/5 min\n",
    "        # imputation and RF imputation are different\n",
    "        train_cols_rf = test_dict['train'][fold]['Ionly_RF'].dropna(how='all', axis=1).columns\n",
    "        test_cols_rf = test_dict['test'][fold]['Ionly_RF'].dropna(how='all', axis=1).columns\n",
    "        cols_to_add_rf = np.setdiff1d(train_cols_rf,test_cols_rf)\n",
    "\n",
    "        for treat in ('Ionly', 'NGP', 'Ionly_RF', 'NGP_RF'):\n",
    "            test_numb = len(test_dict['test'][fold][treat])\n",
    "            df_prep = []\n",
    "            \n",
    "            if treat.endswith('_RF'): # If RF imputation add the respective features to add\n",
    "                for i in cols_to_add_rf:\n",
    "                    df_prep.append(min(test_dict['train'][fold][treat][i]))\n",
    "            else: # If 1/5 min imputation add the respective features to add\n",
    "                for i in cols_to_add:\n",
    "                    df_prep.append(min(test_dict['train'][fold][treat][i]))\n",
    "\n",
    "            df = test_dict['test'][fold][treat].copy()\n",
    "            if treat.endswith('_RF'):\n",
    "                df = pd.concat((test_dict['test'][fold][treat], \n",
    "                                pd.DataFrame((df_prep, ) * test_numb, \n",
    "                                             index=df.index, columns=cols_to_add_rf)), axis=1)\n",
    "            else:\n",
    "                df = pd.concat((test_dict['test'][fold][treat], \n",
    "                                pd.DataFrame((df_prep, ) * test_numb, \n",
    "                                             index=df.index, columns=cols_to_add)), axis=1)\n",
    "            test_dict['test'][fold][treat] = df[\n",
    "                test_dict['train'][fold][treat].columns] # Order the \n",
    "\n",
    "        end2 = perf_counter()\n",
    "        print('Fold number', fold, f' done! took {(end2 - start2):.3f} s!!')\n",
    "        fold = fold+1\n",
    "\n",
    "    #ds['iter_fold_splits'][n_iter+1] = test_dict\n",
    "    end = perf_counter()\n",
    "    print(name, n_iter+1, f' iteration done! took {(end - start):.3f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e1478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform float columns (names of features) into strings so scikit-learn functions can handle them better\n",
    "for name, ds in datasets.items():\n",
    "    print(name)\n",
    "    for treat in ('data', 'Ionly', 'NGP', 'Ionly_RF', 'NGP_RF'):\n",
    "        ds[treat].columns = [str(col) for col in ds[treat].columns]\n",
    "        # Only HD has train and test keys\n",
    "        if name == 'HD':\n",
    "            ds['train'][treat].columns = [str(col) for col in ds['train'][treat].columns]\n",
    "            ds['test'][treat].columns = [str(col) for col in ds['test'][treat].columns]\n",
    "\n",
    "    ds_iter = ds['iter_fold_splits']\n",
    "    for itr in range(len(ds_iter.keys())):\n",
    "        for fold in ds_iter[itr+1]['train'].keys():\n",
    "            for treat in ('data', 'Ionly', 'NGP', 'Ionly_RF', 'NGP_RF'):\n",
    "                ds_iter[itr+1]['train'][fold][treat].columns = [\n",
    "                    str(col) for col in ds_iter[itr+1]['train'][fold][treat].columns]\n",
    "                ds_iter[itr+1]['test'][fold][treat].columns = [\n",
    "                    str(col) for col in ds_iter[itr+1]['test'][fold][treat].columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85fcbb2",
   "metadata": {},
   "source": [
    "### Generate json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cbb9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08cd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE:\n",
    "    # ensure dir exists\n",
    "    path = Path.cwd() / \"store_files\"\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    storepath = Path.cwd() / \"store_files\" / 'processed_data.h5'\n",
    "\n",
    "    store = pd.HDFStore(storepath, complevel=9, complib=\"blosc:blosclz\")\n",
    "    #pd.set_option('io.hdf.default_format','table')\n",
    "\n",
    "    # keep json serializable values and store dataFrames in HDF store\n",
    "\n",
    "    serializable = {}\n",
    "    # Store in and h5 store the pandas dataframes created and in json file the rest\n",
    "    # Since we have a lot of nested dicts in 'iter_fold_splits', the save and load the files back up code is a bit complex\n",
    "    # Probably not the best way but functional\n",
    "    # 'AA_' and 'TTS_' are used as special delimiters for the iter_fold_splits and train and test sets keys in the dict\n",
    "    # Since they have the nested dicts to call on them when reading back the files\n",
    "    for dskey, dataset in datasets.items():\n",
    "        serializable[dskey] = {}\n",
    "        for key, value in dataset.items():\n",
    "            #print(dskey, key)\n",
    "            \n",
    "            # Save DataFrames\n",
    "            if isinstance(value, pd.DataFrame):\n",
    "                storekey = dskey + '_' + key\n",
    "                #print('-----', storekey)\n",
    "                store[storekey] = value\n",
    "                serializable[dskey][key] = f\"INSTORE_{storekey}\"\n",
    "            \n",
    "            # Save DataFrames in nested dicts of the iter fold splits\n",
    "            elif key == 'iter_fold_splits':\n",
    "                for iteration, i in value.items():\n",
    "                    for group, n in i.items():\n",
    "                        for fold, j in n.items():\n",
    "                            #print(j)\n",
    "                            for treat, dfs in j.items():\n",
    "                                storekey = dskey + '_' + key + 'AA_' + str(iteration) + '_' + group + '_' + str(fold) + '_' + treat\n",
    "                                if treat == 'target':\n",
    "                                    serializable[dskey][storekey] = dfs\n",
    "                                #print(df)\n",
    "                                else:\n",
    "                                    store[storekey] = dfs\n",
    "                                    serializable[dskey][storekey] = f\"INSTORE_{storekey}\"\n",
    "            \n",
    "            # Save DataFrames in nested dict for train and test group of the HD\n",
    "            elif key in ('train','test'):\n",
    "                for treat, dfs in value.items():\n",
    "                    storekey = dskey + '_' + key + 'TTS_' + treat\n",
    "                    if treat == 'target':\n",
    "                        serializable[dskey][storekey] = dfs\n",
    "                    #print(df)\n",
    "                    else:\n",
    "                        store[storekey] = dfs\n",
    "                        serializable[dskey][storekey] = f\"INSTORE_{storekey}\"\n",
    "            else:\n",
    "                serializable[dskey][key] = value\n",
    "    store.close()\n",
    "\n",
    "\n",
    "    path = path / 'processed_data.json'\n",
    "    with open(path, \"w\", encoding='utf8') as write_file:\n",
    "        json.dump(serializable, write_file)\n",
    "\n",
    "    #serializable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a1a08",
   "metadata": {},
   "source": [
    "#### Reading back the files if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba3d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the datasets are\n",
    "path = Path.cwd() / \"store_files\" / 'processed_data.json'\n",
    "storepath = Path.cwd() / \"store_files\" / 'processed_data.h5'\n",
    "with pd.HDFStore(storepath) as store:\n",
    "    \n",
    "    # Read into a dictionary not DataFrame data\n",
    "    with open(path, encoding='utf8') as read_file:\n",
    "        datasets = json.load(read_file)\n",
    "    \n",
    "    # Add DataFrame data to dict\n",
    "    for dskey, dataset in datasets.items():\n",
    "        dataset['iter_fold_splits'] = {}\n",
    "        if dskey == 'HD':\n",
    "            dataset['train'] = {}\n",
    "            dataset['test'] = {}\n",
    "        for key in dataset:\n",
    "            # Created right before\n",
    "            if 'iter_fold_splits' == key:\n",
    "                continue\n",
    "            value = dataset[key]\n",
    "            if isinstance(value, str) and value.startswith(\"INSTORE\"):\n",
    "                storekey = value.split(\"_\", 1)[1]\n",
    "                #print(storekey)\n",
    "                # Load the data from 'iter_fold_splits' carefully restoring the nested dictionaries\n",
    "                if len(storekey.split(\"AA_\")) > 1: # This separation was made to identify the 'iter_fold_splits' data\n",
    "                    dictkeys = (storekey.split(\"AA_\")[1]).split('_',3)\n",
    "                    # Create nested dicts\n",
    "                    if int(dictkeys[0]) not in dataset['iter_fold_splits'].keys():\n",
    "                        dataset['iter_fold_splits'][int(dictkeys[0])] = {}\n",
    "                    if dictkeys[1] not in dataset['iter_fold_splits'][int(dictkeys[0])].keys():\n",
    "                        dataset['iter_fold_splits'][int(dictkeys[0])][dictkeys[1]] = {}\n",
    "                    if int(dictkeys[2]) not in dataset['iter_fold_splits'][int(dictkeys[0])][dictkeys[1]].keys():\n",
    "                        dataset['iter_fold_splits'][int(dictkeys[0])][dictkeys[1]][int(dictkeys[2])] = {}\n",
    "                    dataset['iter_fold_splits'][int(dictkeys[0])][dictkeys[1]][int(dictkeys[2])][dictkeys[3]] = store[storekey]\n",
    "                \n",
    "                # Load the data from 'train' and 'test' from HD dataset keys carefully restoring the nested dictionaries\n",
    "                elif len(storekey.split(\"TTS_\")) > 1:\n",
    "                    dictkeys = ((storekey.split(\"TTS_\")[0]).split('_')[-1], storekey.split(\"TTS_\")[1])#.split('_',2)\n",
    "                    dataset[dictkeys[0]][dictkeys[1]] = store[storekey]\n",
    "                # Normal DataFrames\n",
    "                else:\n",
    "                    dataset[key] = store[storekey]\n",
    "\n",
    "            # convert colors to tuples, since they are read as lists from json file\n",
    "            elif key == 'label_colors':\n",
    "                dataset[key] = {lbl: tuple(c) for lbl, c in value.items()}\n",
    "            elif key == 'sample_colors':\n",
    "                dataset[key] = [tuple(c) for c in value]\n",
    "            elif key.endswith('target') and key.startswith(dskey):\n",
    "                if len(key.split(\"AA_\")) > 1: \n",
    "                    dictkeys = ((key.split(\"_\", 1)[1]).split(\"AA_\")[1]).split('_',3)\n",
    "                    dataset['iter_fold_splits'][int(dictkeys[0])][dictkeys[1]][int(dictkeys[2])][dictkeys[3]] = value\n",
    "                else:\n",
    "                    dictkeys = ((key.split(\"TTS_\")[0]).split('_')[-1], key.split(\"TTS_\")[1])#.split('_',2)\n",
    "                    dataset[dictkeys[0]][dictkeys[1]] = value\n",
    "\n",
    "# Remove extra keys\n",
    "for name, ds in datasets.items():\n",
    "    keys_to_remove = [keys for keys in ds.keys() if keys.startswith(name)]\n",
    "    for key in keys_to_remove:\n",
    "        ds.pop(key)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
