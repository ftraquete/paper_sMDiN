{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5edad76a",
   "metadata": {},
   "source": [
    "# Sample Mass-Difference Networks in Metabolomics Data Analysis\n",
    "\n",
    "Notebook to support the study on the application of **Sample M**ass-**Di**fference **N**etworks as a highly specific competing form of pre-processing procedure for high-resolution metabolomics data.\n",
    "\n",
    "Mass-Difference Networks are focused into making networks from a list of masses. Each _m/z_ will represent a node. Nodes will be connected if the difference in their masses can be associated to a simple chemical reaction (enzymatic or non-enzymatic) that led to a change in the elemental composition of its metabolite.\n",
    "\n",
    "The set of mass differences used to build said networks are called a set of MDBs - Mass-Difference-based Building block.\n",
    "\n",
    "This is notebook `paper_sMDiNs_sMDiNsAnalysis.ipynb`\n",
    "\n",
    "## Note (Paper Correction): \n",
    "#### From the MDB list used and the results obtained in the aforementioned paper, the mass used for hydroxylation (CHOH) was 29.002740 Da (CHO) instead of the correct 30.010565 Da. We apologize for this mistake shown in Table 2 in the paper. However, it is worth noting that the conclusions of the paper remain inaltered and this change only leads to minor differences in the figures of the results. \n",
    "\n",
    "Here, we corrected the mass in the file 'transgroups.txt' and 'transgroups_YD.txt' to the correct 30.010565 Da. Thus, the results obtained will not be exactly equal to the figures in the paper. To build the MDiNs to use this method, this corrected mass should be used.\n",
    "\n",
    "\n",
    "## Organization of the Notebook\n",
    "\n",
    "- Loading up pre-processed and pre-treated dataset databases.\n",
    "- **Reading built MDiNs from Cytoscape and analyse some of their characteristics.**\n",
    "- **Subgraphing Sample MDiNs from MDiNs and analyse them.**\n",
    "- Join results from sMDiN analysis to the database of pre-processed and pre-treated datasets.\n",
    "\n",
    "Warning: Run this notebook after `paper_sMDiNs_database_prep.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd8e5fe",
   "metadata": {},
   "source": [
    "#### Needed Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ed056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.spatial.distance as dist\n",
    "import scipy.cluster.hierarchy as hier\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import ticker\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.ensemble as skensemble\n",
    "\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "# Metabolinks package\n",
    "import metabolinks as mtl\n",
    "import metabolinks.transformations as transf\n",
    "\n",
    "# Python files in the repository\n",
    "import multianalysis as ma\n",
    "from elips import plot_confidence_ellipse\n",
    "\n",
    "# For multiprocessing sMDiN analysis of the HD dataset\n",
    "import smdins\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12da40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json for persistence\n",
    "\n",
    "import json\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88df4c",
   "metadata": {},
   "source": [
    "## Description of dataset records\n",
    "\n",
    "`datasets` is the global dict that holds all data sets. It is a **dict of dict's**.\n",
    "\n",
    "Each data set is **represented as a dict**.\n",
    "\n",
    "Each record has the following fields (keys):\n",
    "\n",
    "- `name`: the table/figure name of the data set\n",
    "- `source`: the biological source for each dataset\n",
    "- `mode`: the aquisition mode\n",
    "- `alignment`: the alignment used to generate the data matrix\n",
    "- `data`: the data matrix\n",
    "- `target`: the sample labels, possibly already integer encoded\n",
    "- `MDiN`: Mass-Difference Network - Not present here, only on sMDiNsAnalysis notebook\n",
    "- `<treatment name>`: transformed data matrix / network. These treatment names can be\n",
    "    - `Ionly`: missing value imputed data by 1/5 of the minimum value in each sample in the dataset, only\n",
    "    - `NGP`: normalized, glog transformed and Pareto scaled\n",
    "    - `Ionly_RF`: missing value imputed data by random forests, only\n",
    "    - `NGP_RF`: normalized, glog transformed and Pareto scaled\n",
    "    - `IDT`: `NGP_RF` or `NGP` - Intensity-based Data pre-Treatment chosen as comparison based on which of the two performed better for each dataset and each statistical method\n",
    "    - `sMDiN`: Sample Mass-Difference Networks - Not present here, only on sMDiNsAnalysis notebook\n",
    "       \n",
    "- `<sMDiN analysis name>`: data matrix from nework analysis of MDiNs - Not in this notebook\n",
    "    - `Degree`: degree analysis of each sMDiN\n",
    "    - `Betweenness`: betweenness centrality analysis of each sMDiN\n",
    "    - `Closeness`: closeness centrality of analysis of each sMDiN\n",
    "    - `MDBI`: analysis on the impact of each MDB (Mass-Difference based building-block) on building each sMDiN\n",
    "    - `GCD11`: Graphlet Correlation Distance of 11 different orbits (maximum of 4-node graphlets) between each sMDiN.\n",
    "    - `WMDBI`: an alternative calculation of MDBI using the results from the degree analysis.\n",
    "\n",
    "- `iter_fold_splits`: contains nested dicts that identify and contain each transformed training and testing groups data matrices with their respective iteration, training/test, fold number and one of the previously mentioned data pre-treatments\n",
    "- `train`: specific to the HD dataset; contains a set of the different pre-treatments and sMDin analysis mentioned and a target based on the training set defined for HD\n",
    "- `test`: specific to the HD dataset; contains a set of the different pre-treatments and sMDin analysis mentioned and a target based on the external test set defined for HD\n",
    "\n",
    "\n",
    "The keys of `datasets` may be shared with dicts holding records resulting from comparison analysis.\n",
    "\n",
    "Here are the keys (and respective names) of datasets used in this study:\n",
    "\n",
    "- GD_global2 (GDg2)\n",
    "- GD_class2 (GDc2)\n",
    "- YD (YD)\n",
    "- vitis_types (GD types)\n",
    "- HD (HD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014b062b",
   "metadata": {},
   "source": [
    "### Reading datasets database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312cffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the datasets are\n",
    "path = Path.cwd() / \"store_files\" / 'processed_data.json'\n",
    "storepath = Path.cwd() / \"store_files\" / 'processed_data.h5'\n",
    "with pd.HDFStore(storepath) as store:\n",
    "    \n",
    "    # Read into a dictionary not DataFrame data\n",
    "    with open(path, encoding='utf8') as read_file:\n",
    "        datasets = json.load(read_file)\n",
    "    \n",
    "    # Add DataFrame data to dict\n",
    "    for dskey, dataset in datasets.items():\n",
    "        dataset['iter_fold_splits'] = {}\n",
    "        if dskey == 'HD':\n",
    "            dataset['train'] = {}\n",
    "            dataset['test'] = {}\n",
    "        for key in dataset:\n",
    "            # Created right before\n",
    "            if 'iter_fold_splits' == key:\n",
    "                continue\n",
    "            value = dataset[key]\n",
    "            if isinstance(value, str) and value.startswith(\"INSTORE\"):\n",
    "                storekey = value.split(\"_\", 1)[1]\n",
    "                #print(storekey)\n",
    "                # Load the data from 'iter_fold_splits' carefully restoring the nested dictionaries\n",
    "                if len(storekey.split(\"AA_\")) > 1: # This separation was made to identify the 'iter_fold_splits' data\n",
    "                    dictkeys = (storekey.split(\"AA_\")[1]).split('_',3)\n",
    "                    # Create nested dicts\n",
    "                    if int(dictkeys[0]) not in dataset['iter_fold_splits'].keys():\n",
    "                        dataset['iter_fold_splits'][int(dictkeys[0])] = {}\n",
    "                    if dictkeys[1] not in dataset['iter_fold_splits'][int(dictkeys[0])].keys():\n",
    "                        dataset['iter_fold_splits'][int(dictkeys[0])][dictkeys[1]] = {}\n",
    "                    if int(dictkeys[2]) not in dataset['iter_fold_splits'][int(dictkeys[0])][dictkeys[1]].keys():\n",
    "                        dataset['iter_fold_splits'][int(dictkeys[0])][dictkeys[1]][int(dictkeys[2])] = {}\n",
    "                    dataset['iter_fold_splits'][int(dictkeys[0])][dictkeys[1]][int(dictkeys[2])][dictkeys[3]] = store[storekey]\n",
    "                \n",
    "                # Load the data from 'train' and 'test' from HD dataset keys carefully restoring the nested dictionaries\n",
    "                elif len(storekey.split(\"TTS_\")) > 1:\n",
    "                    dictkeys = ((storekey.split(\"TTS_\")[0]).split('_')[-1], storekey.split(\"TTS_\")[1])#.split('_',2)\n",
    "                    dataset[dictkeys[0]][dictkeys[1]] = store[storekey]\n",
    "                # Normal DataFrames\n",
    "                else:\n",
    "                    dataset[key] = store[storekey]\n",
    "\n",
    "            # convert colors to tuples, since they are read as lists from json file\n",
    "            elif key == 'label_colors':\n",
    "                dataset[key] = {lbl: tuple(c) for lbl, c in value.items()}\n",
    "            elif key == 'sample_colors':\n",
    "                dataset[key] = [tuple(c) for c in value]\n",
    "            elif key.endswith('target') and key.startswith(dskey):\n",
    "                if len(key.split(\"AA_\")) > 1: \n",
    "                    dictkeys = ((key.split(\"_\", 1)[1]).split(\"AA_\")[1]).split('_',3)\n",
    "                    dataset['iter_fold_splits'][int(dictkeys[0])][dictkeys[1]][int(dictkeys[2])][dictkeys[3]] = value\n",
    "                else:\n",
    "                    dictkeys = ((key.split(\"TTS_\")[0]).split('_')[-1], key.split(\"TTS_\")[1])#.split('_',2)\n",
    "                    dataset[dictkeys[0]][dictkeys[1]] = value\n",
    "\n",
    "# Remove extra keys\n",
    "for name, ds in datasets.items():\n",
    "    keys_to_remove = [keys for keys in ds.keys() if keys.startswith(name)]\n",
    "    for key in keys_to_remove:\n",
    "        ds.pop(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atomic masses - https://ciaaw.org/atomic-masses.htm\n",
    "#Isotopic abundances-https://ciaaw.org/isotopic-abundances.htm/https://www.degruyter.com/view/journals/pac/88/3/article-p293.xml\n",
    "# Isotopic abundances from Pure Appl. Chem. 2016; 88(3): 293–306,\n",
    "# Isotopic compositions of the elements 2013 (IUPAC Technical Report), doi: 10.1515/pac-2015-0503\n",
    "\n",
    "chemdict = {'H':(1.0078250322, 0.999844),\n",
    "            'C':(12.000000000, 0.988922),\n",
    "            'N':(14.003074004, 0.996337),\n",
    "            'O':(15.994914619, 0.9976206),\n",
    "            'Na':(22.98976928, 1.0),\n",
    "            'P':(30.973761998, 1.0),\n",
    "            'S':(31.972071174, 0.9504074),\n",
    "            'Cl':(34.9688527, 0.757647),\n",
    "            'F':(18.998403163, 1.0),\n",
    "            'C13':(13.003354835, 0.011078) # Carbon 13 isotope\n",
    "           } \n",
    "\n",
    "# electron mass from NIST http://physics.nist.gov/cgi-bin/cuu/Value?meu|search_for=electron+mass\n",
    "electron_mass = 0.000548579909065"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65385b1a",
   "metadata": {},
   "source": [
    "### Loading Mass-Difference Network of benchmark datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a227a",
   "metadata": {},
   "source": [
    "#### MDBs (Mass-Difference-based Building blocks) \n",
    "\n",
    "Building the **list of MDBs** to use when building the MDiNs.\n",
    "\n",
    "MDB - Mass-Difference-based Building blocks.\n",
    "\n",
    "The choice of this MDBs is then crucial in the network building process. Ideally, they should represent different 'simple' biochemical reactions that cover the most common and ubiquitous enzymatic and non-enzymatic reactions while also being a relatively small amount of reactions – a total of 15 were picked - and maintaining the metabolite formula charge neutrality. \n",
    "\n",
    "For example, to maintain neutrality, a phosphorylation would mean the overall addition of a PO3H – addition of a -PO3$^{2-}$ group + 2 H$^+$ (maintaining neutrality) to replace an H atom in a metabolite. All the MDBs chosen represent changes in metabolites of no more than 5 atoms and less than 80 Da (small size). Each MDB should represent a set of chemically known reactions and a change in every main element in metabolites (C, H, O, N, S and P) is represented by at least one of the MDBs. To fulfil these conditions, representative MDBs were searched using BRENDA (https://www.brenda-enzymes.org/). The groups chosen were the following:\n",
    "\n",
    "- CH2 (methylation) \n",
    "- O (oxygenation) \n",
    "- H2 (Hydrogenation)\n",
    "- O(N-H-) (Aminase): NH3(O-) - H2\n",
    "- PO3H (phosphorylation)\n",
    "- NH3(O-) (transaminases)\n",
    "- SO3 (sulphation)\n",
    "- CO (like formylation) \n",
    "- CO2 (carboxylation, decarboxylation)\n",
    "- CHOH (Hydroxymethylation) - **The mass is 30.010565 Da. In the paper, this MDB's mass was calculated as if it was CHO (29.002740 Da), when it should be 30.010565 Da as it is here.**\n",
    "- NCH (formidoyltransferase)\n",
    "- CONH (carbamoyltransferase)\n",
    "- C2H2O (acetylation)\n",
    "- S (rare but an extra S reaction)\n",
    "- H2O\n",
    "\n",
    "### Note:\n",
    "**The mass used for hydroxylation (CHOH) in the paper was 29.002740 Da (CHO) instead of the correct 30.010565 Da used here. We apologize for this inaccuracy. The conclusions of the paper remain inaltered. This correction made here only leads to minor differences in the figures of the results.**\n",
    "\n",
    "\n",
    "For the YD dataset, an extra 2 MDBs were used as an example of specific MDBs that can be used for certain datasets. These two MDBs were added since these reactions are known to occur with higher glycation in a biological system. Despite mainly targeting proteins, some polyssaccarides can also be glycated. Since the yeast mutants gene deletion affect enzymes involved in methylglyoxal metabolism (ΔGLO1, ΔGLO2, ΔGRE3), which is an anti-glycation mechanism in the cell.\n",
    "\n",
    "- CHCOOH (carboxymethylation)\n",
    "- CCH3COOH (carboxyethylation)\n",
    "\n",
    "There could be many other MDB representing other reactions that can also be included such as CN2H2 (amidinotransferases), COCH2COO (malonyl transferases), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b36c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemical Formula transformations (MDBs chosen)\n",
    "MDB = ['H2','CH2','CO2','O','CHOH','NCH','O(N-H-)','S','CONH','PO3H','NH3(O-)','SO3','CO', 'C2H2O', 'H2O']\n",
    "MDB_YD = ['H2','CH2','CO2','O','CHOH','NCH','O(N-H-)','S','CONH','PO3H','NH3(O-)','SO3','CO', 'C2H2O', 'H2O', \n",
    "          'C2H2O2', 'C3H4O2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0d662",
   "metadata": {},
   "source": [
    "### Build the Mass-Difference Networks in Cytoscape using MetaNetter 2.0\n",
    "\n",
    "To build the network in Cytoscape we need:\n",
    "\n",
    "1) A list of neutral masses (with masses as float in node attributes).\n",
    "\n",
    "2) A list of allowed transformations with specific mass differences - list of MDBs that we build above.\n",
    "\n",
    "The transformation files and the dataset files were built in `paper_sMDiNs_database_prep` and used in Cytoscape to build the MDiNs.\n",
    "\n",
    "Parameters: 1 ppm error allowed for edge establishment.\n",
    "\n",
    "The Yeast datasets already have the m/z 'buckets' that are already representing the neutral masses of the metabolites. To consider the neutral masses of the grapevine datasets, a simple transformation by adding or removing a proton (Hydrogen atom mass - electron mass), depending if the ionization mode is negative or positive, respectively, on the m/z peaks was made.\n",
    "\n",
    "Only one full network (for each benchmark dataset) is built. Then, subgraphs of them will be used to select every sample MDiN. This is the same as building an MDiN for each sample.\n",
    "\n",
    "The networks were exported in graphml format that networkX module can read.\n",
    "\n",
    "- Nodes have a standard number ID instead of the mass (stored as the attribute 'mass'). Other attributes stored are irrelevant. \n",
    "- Edges among the different attributes have a very useful attribute called 'Transformation' which stores which MDB of the list was used to establish the edge - will be used for MDB Impact analysis.\n",
    "- Finally, the graph is directed. Since reactions are bidireccional, they will be transformed to undirected graphs.\n",
    "\n",
    "Changes that will be made to the network:\n",
    "\n",
    "- Nodes will be identified by their masses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a9d33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the MDiNs built using Cytoscape's MetaNetter\n",
    "for name, ds in datasets.items():\n",
    "    print(f'Reading MDiN based on data in {name}', end=' ...')\n",
    "    MDiN_temp = nx.read_graphml('mass_data/MassList' + name +'.graphml')\n",
    "    \n",
    "    # Making dicts for the new names\n",
    "    new_nodes = dict.fromkeys(MDiN_temp.nodes(),0)\n",
    "\n",
    "    for i,mz in nx.get_node_attributes(MDiN_temp,'mass').items(): # i is old name, mz is mass/new name\n",
    "        new_nodes[i] = mz\n",
    "\n",
    "    # Relabeling nodes\n",
    "    MDiN_temp = nx.relabel_nodes(MDiN_temp, mapping=new_nodes)\n",
    "    \n",
    "    ds['MDiN'] = MDiN_temp.to_undirected()\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0261221",
   "metadata": {},
   "source": [
    "### MDiN characteristics\n",
    "\n",
    "Building a table with general characteristics about the MDiNs built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b16bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Net_Chara = pd.DataFrame(columns=['# Nodes', '# Edges', 'Biggest Comp. Size', 'Connected Nodes', \n",
    "                                  'Diameter', 'Radius'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccb9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, ds in datasets.items():\n",
    "    Net_Chara.loc[name, '# Nodes'] = len(ds['MDiN'].nodes())\n",
    "    Net_Chara.loc[name, '# Edges'] = len(ds['MDiN'].edges())\n",
    "    \n",
    "    Main_component = ds['MDiN'].subgraph(list(sorted(nx.connected_components(ds['MDiN']), key=len, reverse=True)[0]))\n",
    "    Net_Chara.loc[name, 'Biggest Comp. Size'] = len(Main_component.nodes())\n",
    "    Net_Chara.loc[name, 'Diameter'] = nx.diameter(Main_component)\n",
    "    Net_Chara.loc[name, 'Radius'] = nx.radius(Main_component)\n",
    "    \n",
    "    isolated = 0\n",
    "    for node in ds['MDiN'].degree():\n",
    "        if node[1] == 0:\n",
    "            isolated = isolated + 1\n",
    "    Net_Chara.loc[name, 'Connected Nodes'] = (len(ds['MDiN'].nodes()) - isolated) / len(ds['MDiN'].nodes()) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de059d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Net_Chara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of edges associated with each MDB for each MDiN\n",
    "MDB_counts = {}\n",
    "\n",
    "for name, ds in datasets.items():\n",
    "    MDB_counts[name] = dict.fromkeys(MDB_YD, 0) # MDBs from the transformation list\n",
    "    for i in ds['MDiN'].edges():\n",
    "        MDB_counts[name][ds['MDiN'].edges()[i]['Transformation']] = MDB_counts[name][\n",
    "            ds['MDiN'].edges()[i]['Transformation']] + 1\n",
    "        \n",
    "MDB_counts = pd.DataFrame.from_dict(MDB_counts)\n",
    "MDB_counts.reindex(['O(N-H-)','NH3(O-)','H2','CH2','O','H2O','CO','NCH','CHOH','S','C2H2O','CONH','CO2','SO3','PO3H']).T\n",
    "MDB_counts.to_csv('TableS1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ef78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDB_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d598f0",
   "metadata": {},
   "source": [
    "### Discarding all uninformative isolated nodes from the network, that is, the nodes that do not establish any connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa4c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, ds in datasets.items():\n",
    "    new_node_list = []\n",
    "    for node in ds['MDiN'].degree():\n",
    "        if node[1] > 0:\n",
    "            new_node_list.append(node[0])\n",
    "    ds['MDiN'] = ds['MDiN'].subgraph(new_node_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517444b8",
   "metadata": {},
   "source": [
    "### Building and Analysing Sample MDiNs\n",
    "\n",
    "6 Analysis metrics of the Sample MDiNs were used:\n",
    "\n",
    "- Degree analysis\n",
    "- Betweenness - Betweenness centrality analysis\n",
    "- Closeness - Closeness centrality analysis\n",
    "- MDBI - Mass-Difference based building block Impact analysis\n",
    "- GCD11 - Graphlet Correlation Distance using 11 non-redundant graphlet orbits (maximum of 4-node graphlets) - GCD-11 analysis\n",
    "- WMDBI - Weighted Mass-Difference based building block Impact analysis (modification  on MDB)\n",
    "\n",
    "This analysis encompasses metrics that evaluate different aspects of the networs. \n",
    "\n",
    "Three measures of centrality: degree, centrality closeness, betweenness centrality commonly used to characterize networks. These metrics keep each node as a feature (no feature reduction) with its value for each sample being the respective metric value for each sample MDiN.\n",
    "\n",
    "The last three metrics greatly reduce the amount of features that will be considered by statistical methods to discriminate between the sample MDiNs - one to 15 (17 in YD) and the other to 60.\n",
    "\n",
    "**MDB Impact** is a measure of the impact that each MDB had in establishing a sample MDiN. To that end, counts of the number of edges established due to each MDB are counted in each sample MDiN - each MDB represents a set of chemical reactions. To allow comparison between samples with different number of edges the counts in each sample MDiN are scaled by Pareto Scaling. This analysis was made to see if the relative importance of the MDBs in establishing the networks is characteristic of the class the sample belongs to.\n",
    "\n",
    "**Weighted MDB Impact** focuses on quantifying the impact of each MDB used to build each sMDiN and using that information to discriminate between sMDiNs like MDB Impact. However, instead of assigning the same impact to each edge, it assigns the cumulative importance of its two nodes. The importance of the nodes was considered as their importance to discriminate between sMDiNs as estimated by the Gini Importance of the Random Forest models built from the Degree analysis data. This was chosen since Degree analysis of sMDiNs allowed the best sample discrimination out of all network analysis methods. Furthermore, the information from this analysis is the number of connections each node in the sMDiN makes and thus, the discrimination is made based on the different connections of each node. This means that the most important nodes are the ones who have differing edges between samples of different classes and will contribute to  a greater impact of the corresponding edge MDB.\n",
    "\n",
    "**GCD-11** is an analysis that focuses on the structure and topology of the networks. Briefly, this method considers and counts the number of times each node in the sMDiN is in one of 11 of the 15 possible orbits (4 are redundant - 3, 12, 13, 14) of 2 to 4-node graphlets. This builds a Graphlet Degree Vector for each node, building a dataframe with 11 orbits as columns. The spearman correlation between each pair of the 11 columns is calculated to generate a symmetric 11x11 matrix - the Graphlet Correlation Matrix (GCM) that represents, according to the authors, the signature of the network topology. The distance of the these matrices can be used to compare different networks - the Graphlet Correlation Distance. To use this signature of the topology of each network to discriminate the different sMDiNs, the correlations between each pair of orbits (total of 60) were extracted from the Graphlet Correlation Matrix from each sMDiN to be the features that will be compared against each other and used by the statistical methods. This analysis was made to see if the general topology of the sMDiNs is characteristic of the class the sample belongs to, knowing that this metric has been known to sucessfully discriminate between different networks.\n",
    "\n",
    "Useful papers for detailed explanations on GCD-11 and other related methodologies: \n",
    "- Yaveroğlu ÖN, Malod-Dognin N, Davis D, et al. Revealing the Hidden Language of Complex Networks. Sci Rep. 2014;4(1):4547. doi:10.1038/srep04547\n",
    "- Milenković T, Pržulj N. Uncovering biological network function via graphlet degree signatures. Cancer Inform. 2008;6:257-273. doi:10.4137/cin.s680 - some details on graphlet signatures\n",
    "- Tantardini M, Ieva F, Tajoli L, Piccardi C. Comparing methods for comparing networks. Sci Rep. 2019;9(1):1-19. doi:10.1038/s41598-019-53708-y - many different similar methods to GCD-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ac43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, ds in datasets.items():\n",
    "    sMDiNs = {}\n",
    "    if name != 'HD':\n",
    "        for samp in ds['data'].index:\n",
    "            # Subgraphing sMDiN\n",
    "            idxs = [float(i) for i in ds['data'].T[ds['data'].loc[samp,:].replace({np.nan:0}) != 0].index]\n",
    "            sMDiNs[samp] = ds['MDiN'].subgraph(idxs)\n",
    "\n",
    "            # Storing intensity of feature in sample on the nodes\n",
    "            intensity_attr = dict.fromkeys(sMDiNs[samp].nodes(),0)\n",
    "            #print(intensity_attr)\n",
    "            for i,m in nx.get_node_attributes(sMDiNs[samp],'mass').items():\n",
    "                intensity_attr[i] = {'intensity':ds['data'].loc[samp,str(m)]}\n",
    "            nx.set_node_attributes(sMDiNs[samp],intensity_attr)\n",
    "    \n",
    "    else:\n",
    "        # For HD dataset, where the index of the 2D numerical matrix isn't the masses used for the MDiN.\n",
    "        for samp in ds['data'].index:\n",
    "            # Extracting the mass lists of each sample from the 2D numerical matrix\n",
    "            new_idx = ds['data'].T[ds['data'].loc[samp,:].replace({np.nan:0}) != 0].index\n",
    "            new_idx_final = []\n",
    "            for i in new_idx:\n",
    "                # Taking out a proton of the masses to 'neutralise' the mass.\n",
    "                new_idx_final.append(float(i.split('_')[0]) - chemdict['H'][0] + electron_mass)\n",
    "                \n",
    "            # Subgraphing sMDiN with the list of masses extracted\n",
    "            sMDiNs[samp] = ds['MDiN'].subgraph(new_idx_final)\n",
    "    \n",
    "    # Store all sMDiNs\n",
    "    ds['sMDiNs'] = sMDiNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734c9035",
   "metadata": {},
   "source": [
    "Count number of graphlet orbits for GCD-11 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd88484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_orbits(GG):\n",
    "    \"\"\"Calculates the number of times each node of the network is in each possible (non-redundant) orbit in graphlets (maximum\n",
    "    4 nodes).\n",
    "    \n",
    "    Function is not very efficient, all nodes are passed, every graphlet is 'made' for each node present in it so it is made\n",
    "    multiple times.\n",
    "    \n",
    "       GG: networkx graph;\n",
    "    \n",
    "       returns: dict; dictionary (keys are the nodes) of dictionaries (keys are the orbits and values are the number of times)\n",
    "    \"\"\"\n",
    "    \n",
    "    node_orbits = {} # To store results\n",
    "\n",
    "    for i in GG.nodes():\n",
    "\n",
    "        node_orbits[i] = {} # To store results\n",
    "        orbits = node_orbits[i]\n",
    "\n",
    "        # 2 node graphlets - orbit 0\n",
    "        orbits['0'] = GG.degree(i)\n",
    "\n",
    "        # 3 node graphlets - orbit 1,2 (and 3 redundant)\n",
    "        node_neigh = list(GG.neighbors(i))\n",
    "\n",
    "        # orbit 1 and 4 and 6 and 8 and 9\n",
    "        n_orb = 0\n",
    "        n_orb4 = 0\n",
    "        n_orb6 = 0\n",
    "        n_orb8 = 0\n",
    "        n_orb9 = 0\n",
    "\n",
    "        # orbit 1\n",
    "        for j in node_neigh:\n",
    "            neigh_neigh = list(GG.neighbors(j)) # Neighbours of the neighbour j of i\n",
    "            neigh_neigh.remove(i) # Remove i since i is a neighbour of j\n",
    "            for common in nx.common_neighbors(GG, i, j):\n",
    "                neigh_neigh.remove(common) # Remove common neighbours of i and j\n",
    "            n_orb = n_orb + len(neigh_neigh)\n",
    "\n",
    "\n",
    "            # orbit 4 and 8\n",
    "            for n3 in neigh_neigh:\n",
    "                neigh_neigh_neigh = list(GG.neighbors(n3)) # Neighbours of the neighbour n3 of the neighbour j of i\n",
    "                #neigh_neigh_neigh.remove(j)\n",
    "                #if i in neigh_neigh_neigh:\n",
    "                    #neigh_neigh_neigh.remove(i)     \n",
    "                for common in nx.common_neighbors(GG, j, n3):\n",
    "                    if common in neigh_neigh_neigh:\n",
    "                        neigh_neigh_neigh.remove(common)\n",
    "\n",
    "                for common in nx.common_neighbors(GG, i, n3):\n",
    "                    if common in neigh_neigh_neigh:\n",
    "                        neigh_neigh_neigh.remove(common)\n",
    "                        # orbit 8\n",
    "                        if common != j:\n",
    "                            #print(i,j,n3,common)\n",
    "                            n_orb8 = n_orb8 + 1/2 # always goes in 2 directions so it will always pass like this\n",
    "\n",
    "                n_orb4 = n_orb4 + len(neigh_neigh_neigh)\n",
    "                # print(neigh_neigh_neigh)\n",
    "\n",
    "            # orbit 6 and 9\n",
    "            for u,v in itertools.combinations(neigh_neigh, 2):\n",
    "                if not GG.has_edge(u,v):\n",
    "                    n_orb6 = n_orb6 + 1\n",
    "                else:\n",
    "                    n_orb9 = n_orb9 + 1\n",
    "\n",
    "        orbits['1'] = n_orb\n",
    "\n",
    "        # orbit 2 and 5\n",
    "        n_orb = 0\n",
    "        n_orb5 = 0\n",
    "        for u,v in itertools.combinations(node_neigh, 2):\n",
    "            if not GG.has_edge(u,v):\n",
    "                n_orb = n_orb + 1\n",
    "\n",
    "                # orbit 5\n",
    "                neigh_u = list(GG.neighbors(u))\n",
    "                neigh_u.remove(i)\n",
    "                for common in nx.common_neighbors(GG, i, u):\n",
    "                    neigh_u.remove(common)\n",
    "\n",
    "                neigh_v = list(GG.neighbors(v))\n",
    "                neigh_v.remove(i)\n",
    "                for common in nx.common_neighbors(GG, i, v):\n",
    "                    neigh_v.remove(common)\n",
    "\n",
    "                for common in nx.common_neighbors(GG, v, u):\n",
    "                    if common in neigh_u:\n",
    "                        neigh_u.remove(common)\n",
    "                    if common in neigh_v:\n",
    "                        neigh_v.remove(common) \n",
    "\n",
    "                n_orb5 = n_orb5 + len(neigh_u)\n",
    "                n_orb5 = n_orb5 + len(neigh_v)\n",
    "\n",
    "        orbits['2'] = n_orb\n",
    "\n",
    "        # 4 node graphlets - orbit 4,5,6,7,8,9,10,11 (and 12,13,14 redundant)\n",
    "\n",
    "        # orbit 4\n",
    "        orbits['4'] = n_orb4\n",
    "\n",
    "        # orbit 5\n",
    "        orbits['5'] = n_orb5\n",
    "\n",
    "        # orbit 6\n",
    "        orbits['6'] = n_orb6\n",
    "\n",
    "        # orbit 7 and 11\n",
    "        n_orb = 0\n",
    "        n_orb11 = 0\n",
    "        for u,v,j in itertools.combinations(node_neigh, 3):\n",
    "            n_edge = [GG.has_edge(a,b) for a,b in itertools.combinations((u,v,j), 2)]\n",
    "            #print(sum(n_edge))\n",
    "            if sum(n_edge) == 0:\n",
    "                n_orb = n_orb + 1\n",
    "            elif sum(n_edge) == 1:\n",
    "                n_orb11 = n_orb11 + 1\n",
    "\n",
    "        orbits['7'] = n_orb\n",
    "\n",
    "        # orbit 8\n",
    "        orbits['8'] = int(n_orb8)\n",
    "\n",
    "        # orbit 9\n",
    "        orbits['9'] = n_orb9\n",
    "\n",
    "        # orbit 10\n",
    "        n_orb = 0\n",
    "        for j in node_neigh:\n",
    "            neigh_neigh = list(GG.neighbors(j))\n",
    "            neigh_neigh.remove(i)\n",
    "            for u,v in itertools.combinations(neigh_neigh, 2):\n",
    "                if sum((GG.has_edge(i,u), GG.has_edge(i,v))) == 1:\n",
    "                    if not GG.has_edge(u,v):\n",
    "                        n_orb = n_orb + 1\n",
    "\n",
    "        orbits['10'] = n_orb\n",
    "\n",
    "        # orbit 11\n",
    "        orbits['11'] = n_orb11\n",
    "    \n",
    "    return node_orbits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5ad0a",
   "metadata": {},
   "source": [
    "### **Sample MDiN analysis** and results storage for benchmark datasets (except HD)\n",
    "\n",
    "WMDBI is calculated later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec27c6",
   "metadata": {},
   "source": [
    "**Warning:**  The following warning will be raised: RuntimeWarning: invalid value encountered in true_divide\n",
    "  c /= stddev[:, None].\n",
    "  \n",
    "This happens because the count of a certain orbit is 0 for every single node (equal in every node) and thus the calculation of spearman correlations including that orbit fails and becomes 'nan'. Thus, we give posteriorly a correlation of 0 to those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df96be8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, ds in datasets.items():\n",
    "    \n",
    "    MDBs = MDB\n",
    "    if name == 'HD': # Save HD for later\n",
    "        continue\n",
    "    elif name == 'YD':\n",
    "        MDBs = MDB_YD\n",
    "\n",
    "    print(f'Analysing sample MDiNs from the data in {name}', end=' ...')\n",
    "    \n",
    "    Deg = {}\n",
    "    Betw = {}\n",
    "    Close = {}\n",
    "    MDB_Impact = {}\n",
    "    GCD = {} \n",
    "    \n",
    "    for samp in ds['data'].index:\n",
    "        #print(name, samp)\n",
    "        k = None\n",
    "        \n",
    "        # Centrality measures\n",
    "        Deg[samp] = dict(ds['sMDiNs'][samp].degree())\n",
    "        Betw[samp] = nx.betweenness_centrality(ds['sMDiNs'][samp], k=k)\n",
    "        Close[samp] = nx.closeness_centrality(ds['sMDiNs'][samp])\n",
    "        \n",
    "        # MDB Impact\n",
    "        MDB_Impact[samp] = dict.fromkeys(MDBs, 0) # MDBs from the transformation list\n",
    "        for i in ds['sMDiNs'][samp].edges():\n",
    "            MDB_Impact[samp][ds['sMDiNs'][samp].edges()[i]['Transformation']] = MDB_Impact[samp][\n",
    "                ds['sMDiNs'][samp].edges()[i]['Transformation']] + 1\n",
    "            \n",
    "        # GCD-11\n",
    "        # Corr_Mat\n",
    "        orbits_t = calculating_orbits(ds['sMDiNs'][samp]) # Calculating orbit number for each node\n",
    "        orbits_df = pd.DataFrame.from_dict(orbits_t).T # Transforming into a dataframe\n",
    "        \n",
    "        # Signature matrices\n",
    "        corrMat_ar = stats.spearmanr(orbits_df)[0] # Calculating spearman correlation to obtain 11x11 signature of the network - GCM\n",
    "        corrMat_tri = np.triu(corrMat_ar, k=0) # Both parts of the matrix are equal, so reducing the info to the upper triangle\n",
    "        \n",
    "        # Pulling the signature orbit n (u) - orbit m (v) correlations from the upper triangular matrix of the GCM\n",
    "        # Making the signature of the sample MDiN into a column of the dataset\n",
    "        samp_col = {}\n",
    "        orbits = [0,1,2,4,5,6,7,8,9,10,11]\n",
    "        for u in range(len(corrMat_tri)):\n",
    "            for v in range(u+1, len(corrMat_tri)):\n",
    "                samp_col[str(orbits[u]) + '-' + str(orbits[v])] = corrMat_tri[u,v]\n",
    "            GCD[samp] = samp_col\n",
    "    \n",
    "    # Centrality Measures\n",
    "    ds['Degree'] = pd.DataFrame.from_dict(Deg).replace({np.nan:0}).T\n",
    "    ds['Betweenness'] = pd.DataFrame.from_dict(Betw).replace({np.nan:0}).T\n",
    "    ds['Closeness'] = pd.DataFrame.from_dict(Close).replace({np.nan:0}).T\n",
    "    \n",
    "    # MDB Impact\n",
    "    ds['MDBI'] = pd.DataFrame.from_dict(MDB_Impact).replace({np.nan:0})\n",
    "    #ds['MDB_Imp'] = transf.pareto_scale(ds['MDB_Imp']).T.replace({np.nan:0})\n",
    "    ds['MDBI'] = (ds['MDBI']/ds['MDBI'].sum()).T\n",
    "    \n",
    "    # GCD-11\n",
    "    ds['GCD11'] = pd.DataFrame.from_dict(GCD).replace({np.nan:0}).T\n",
    "    \n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c69c8cd",
   "metadata": {},
   "source": [
    "#### Sample MDiN analysis for the HD dataset using multiprocessing\n",
    "\n",
    "6 cores being used - Warning: see if the pc has at least 6 cores to process.\n",
    "\n",
    "Store results in results_HD list.\n",
    "\n",
    "Function in `smdins.py` called `HD_sMDiN_analysis` performs the 6 network analysis methods as in the cell before but stores them per sample instead of per metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebec88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of processors: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5519c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nº of cores to use\n",
    "with mp.Pool(6) as p:\n",
    "    results_HD = []\n",
    "    for i in tqdm(\n",
    "            p.imap(smdins.HD_sMDiN_analysis,\n",
    "                   [(samp, datasets['HD']['sMDiNs'][samp]) for samp in datasets['HD']['data'].index]),#, chunksize=5),\n",
    "            total=len([samp for samp in datasets['HD']['data'].index])\n",
    "        ):\n",
    "        results_HD.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc49473",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_HD[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming results_HD list with a dict of the results for the metrics for each sample to add to the dataset database\n",
    "\n",
    "Deg = {}\n",
    "Betw = {}\n",
    "Close = {}\n",
    "MDB_Impact = {}\n",
    "GCD = {}\n",
    "\n",
    "for results in results_HD:\n",
    "    Deg[results['Name']] = results['Deg']\n",
    "    Betw[results['Name']] = results['Betw']\n",
    "    Close[results['Name']] = results['Closeness']\n",
    "    MDB_Impact[results['Name']] = results['MDB_Impact']\n",
    "    GCD[results['Name']] = results['GCD']\n",
    "    \n",
    "# Centrality Measures\n",
    "datasets['HD']['Degree'] = pd.DataFrame.from_dict(Deg).replace({np.nan:0}).T\n",
    "datasets['HD']['Betweenness'] = pd.DataFrame.from_dict(Betw).replace({np.nan:0}).T\n",
    "datasets['HD']['Closeness'] = pd.DataFrame.from_dict(Close).replace({np.nan:0}).T\n",
    "\n",
    "# MDB Impact\n",
    "datasets['HD']['MDBI'] = pd.DataFrame.from_dict(MDB_Impact).replace({np.nan:0})\n",
    "#ds['MDBI'] = transf.pareto_scale(ds['MDBI']).T.replace({np.nan:0})\n",
    "datasets['HD']['MDBI'] = (datasets['HD']['MDBI']/datasets['HD']['MDBI'].sum()).T\n",
    "\n",
    "# GCD-11\n",
    "datasets['HD']['GCD11'] = pd.DataFrame.from_dict(GCD).replace({np.nan:0}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8206fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['HD'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffc7a01",
   "metadata": {},
   "source": [
    "#### Transforming Feature names from floats into strings (for scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b681949",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, ds in datasets.items():\n",
    "    print(name)\n",
    "    for treat in ('Degree', 'Betweenness', 'Closeness'):\n",
    "        #print(len(ds[treat]))\n",
    "        ds[treat].columns = [str(col) for col in ds[treat].columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d81f10",
   "metadata": {},
   "source": [
    "### WMDBI (Weighted MDB Impact)\n",
    "\n",
    "To avoid data leakage, importance of the nodes was taken from random forests based on the train dataset of each iteration and each fold of each of the benchmark datasets. As such, this is the only network analysis methods where the information of each sample is not only dependent on its own sMDiN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e47f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First general WMDBI considering all samples of the dataset (for unsupervised analysis)\n",
    "\n",
    "np.random.seed(1357) # Set the seed\n",
    "for name, ds in datasets.items():\n",
    "\n",
    "    print(f'Analysing sample MDiNs from the data in {name}', end=' ...')\n",
    "    \n",
    "    MDBs = MDB\n",
    "    if name == 'YD':\n",
    "        MDBs = MDB_YD\n",
    "\n",
    "    Weighted_MDB_Impact = {}\n",
    "\n",
    "    n_fold = 3\n",
    "    if name in ('vitis_types', 'HD'):\n",
    "        n_fold = 5\n",
    "\n",
    "    # Build Random Forest model based on the degree analysis of the dataset\n",
    "    degree_rf = ma.RF_model(ds['Degree'], ds['target'], n_trees=100, n_fold=n_fold)\n",
    "\n",
    "    # Series with the importance of each node (ordered from most to least important)\n",
    "    ordered_imp_feats = pd.Series(\n",
    "        degree_rf['model'].feature_importances_, index=ds['Degree'].columns).sort_values(ascending=False)\n",
    "\n",
    "    # For each sample / sMDiN\n",
    "    for samp in ds['data'].index:\n",
    "\n",
    "        # Weighted MDB Impact\n",
    "        Weighted_MDB_Impact[samp] = dict.fromkeys(MDBs, 0) # MDBs from the transformation list\n",
    "        for node in ds['sMDiNs'][samp].nodes(): # See each node\n",
    "            for edge in ds['sMDiNs'][samp].edges(node): # And each edge associated with that node\n",
    "                # See the corresponding MDB and add the importance of the node to the impact of that MDB\n",
    "\n",
    "                #print(node, ds['sMDiNs'][samp].edges()[edge]['Transformation'], ordered_imp_feats.loc[node])\n",
    "                Weighted_MDB_Impact[samp][ds['sMDiNs'][samp].edges()[edge][\n",
    "                    'Transformation']] += ordered_imp_feats.loc[str(node)]\n",
    "\n",
    "    # Weighted MDB Impact\n",
    "    ds['WMDBI'] = pd.DataFrame.from_dict(Weighted_MDB_Impact).replace({np.nan:0}).T\n",
    "\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962b8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the WMDBI for each iteration/fold combination using only the samples of the training set of each fold\n",
    "# In order to avoid data bleeding\n",
    "\n",
    "np.random.seed(1357) # Set the seed\n",
    "for name, ds in datasets.items():\n",
    "    \n",
    "    print(f'Analysing sample MDiNs from the data in {name} ...')\n",
    "    \n",
    "    MDBs = MDB\n",
    "    if name == 'YD':\n",
    "        MDBs = MDB_YD\n",
    "\n",
    "    ds_iter = ds['iter_fold_splits']\n",
    "\n",
    "    for itr in range(len(ds_iter.keys())):\n",
    "        \n",
    "        # Fit a Random Forest model for each fold in stratified k-fold cross validation\n",
    "        for fold in ds_iter[itr+1]['train'].keys():\n",
    "            ds_iter[itr+1]['train'][fold]['Degree'] = ds['Degree'].loc[ds_iter[itr+1]['train'][fold]['data'].index]\n",
    "            \n",
    "            # Build Random Forest model based on the degree analysis of the dataset\n",
    "            # Considering only samples of the training set\n",
    "            # n_fold here is irrelevant, it is not used that data\n",
    "            degree_rf = ma.RF_model(ds_iter[itr+1]['train'][fold]['Degree'], ds_iter[itr+1]['train'][fold]['target'], \n",
    "                                    n_trees=100, n_fold=2)\n",
    "            \n",
    "            # Series with the importance of each node (ordered from most to least important)\n",
    "            ordered_imp_feats = pd.Series(degree_rf['model'].feature_importances_, \n",
    "                                        index=ds_iter[itr+1]['train'][fold]['Degree'].columns).sort_values(ascending=False)\n",
    "            \n",
    "            Weighted_MDB_Impact = {}\n",
    "            \n",
    "            #print(ds_iter[itr+1]['train'][fold]['Degree'])\n",
    "            # For each sample / sMDiN of the training samples\n",
    "            for samp in ds_iter[itr+1]['train'][fold]['Degree'].index:\n",
    "\n",
    "                # Weighted MDB Impact\n",
    "                Weighted_MDB_Impact[samp] = dict.fromkeys(MDBs, 0) # MDBs from the transformation list\n",
    "                for node in ds['sMDiNs'][samp].nodes(): # See each node\n",
    "                    for edge in ds['sMDiNs'][samp].edges(node): # And each edge associated with that node\n",
    "                        # See the corresponding MDB and add the importance of the node to the impact of that MDB\n",
    "\n",
    "                        #print(node, ds['sMDiNs'][samp].edges()[edge]['Transformation'], ordered_imp_feats.loc[node])\n",
    "                        Weighted_MDB_Impact[samp][ds['sMDiNs'][samp].edges()[edge][\n",
    "                            'Transformation']] += ordered_imp_feats.loc[str(node)]\n",
    "            \n",
    "            # Weighted MDB Impact\n",
    "            ds_iter[itr+1]['train'][fold]['WMDBI'] = pd.DataFrame.from_dict(Weighted_MDB_Impact).replace({np.nan:0}).T\n",
    "            \n",
    "            Weighted_MDB_Impact = {}\n",
    "            \n",
    "            # For each sample / sMDiN of the test samples\n",
    "            for samp in ds_iter[itr+1]['test'][fold]['data'].index:\n",
    "\n",
    "                # Weighted MDB Impact\n",
    "                Weighted_MDB_Impact[samp] = dict.fromkeys(MDBs, 0) # MDBs from the transformation list\n",
    "                for node in ds['sMDiNs'][samp].nodes(): # See each node\n",
    "                    for edge in ds['sMDiNs'][samp].edges(node): # And each edge associated with that node\n",
    "                        # See the corresponding MDB and add the importance of the node to the impact of that MDB\n",
    "\n",
    "                        #print(node, ds['sMDiNs'][samp].edges()[edge]['Transformation'], ordered_imp_feats.loc[node])\n",
    "                        Weighted_MDB_Impact[samp][ds['sMDiNs'][samp].edges()[edge][\n",
    "                            'Transformation']] += ordered_imp_feats.loc[str(node)]\n",
    "            \n",
    "            # Weighted MDB Impact\n",
    "            ds_iter[itr+1]['test'][fold]['WMDBI'] = pd.DataFrame.from_dict(Weighted_MDB_Impact).replace({np.nan:0}).T\n",
    "\n",
    "        #print('Iteration', itr+1)\n",
    "    \n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c491e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally for the external test data and training data of HD dataset calculate WMDBI based on the RF model of the training\n",
    "# set\n",
    "\n",
    "np.random.seed(1357)\n",
    "name, ds = 'HD', datasets['HD']\n",
    "    \n",
    "print(f'Analysing sample MDiNs from the data in the training and external test set in {name}', end=' ...')\n",
    "\n",
    "MDBs = MDB\n",
    "\n",
    "ds['train']['Degree'] = ds['Degree'].loc[ds['train']['data'].index]\n",
    "\n",
    "# Random Forest setup and fit\n",
    "# n_fold here is irrelevant, it is not used that data\n",
    "degree_rf = ma.RF_model(ds['train']['Degree'], ds['train']['target'], \n",
    "                        n_trees=100, n_fold=2)\n",
    "\n",
    "ordered_imp_feats = pd.Series(degree_rf['model'].feature_importances_, \n",
    "                            index=ds['train']['Degree'].columns).sort_values(ascending=False)\n",
    "\n",
    "Weighted_MDB_Impact = {}\n",
    "\n",
    "# For each sample / sMDiN of the training samples\n",
    "for samp in ds['train']['Degree'].index:\n",
    "\n",
    "    # Weighted MDB Impact\n",
    "    Weighted_MDB_Impact[samp] = dict.fromkeys(MDBs, 0) # MDBs from the transformation list\n",
    "    for node in ds['sMDiNs'][samp].nodes(): # See each node\n",
    "        for edge in ds['sMDiNs'][samp].edges(node): # And each edge associated with that node\n",
    "            # See the corresponding MDB and add the importance of the node to the impact of that MDB\n",
    "\n",
    "            #print(node, ds['sMDiNs'][samp].edges()[edge]['Transformation'], ordered_imp_feats.loc[node])\n",
    "            Weighted_MDB_Impact[samp][ds['sMDiNs'][samp].edges()[edge][\n",
    "                'Transformation']] += ordered_imp_feats.loc[str(node)]\n",
    "\n",
    "# Weighted MDB Impact\n",
    "ds['train']['WMDBI'] = pd.DataFrame.from_dict(Weighted_MDB_Impact).replace({np.nan:0}).T\n",
    "\n",
    "Weighted_MDB_Impact = {}\n",
    "\n",
    "# For each sample / sMDiN of the test samples\n",
    "for samp in ds['test']['data'].index:\n",
    "\n",
    "    # Weighted MDB Impact\n",
    "    Weighted_MDB_Impact[samp] = dict.fromkeys(MDBs, 0) # MDBs from the transformation list\n",
    "    for node in ds['sMDiNs'][samp].nodes(): # See each node\n",
    "        for edge in ds['sMDiNs'][samp].edges(node): # And each edge associated with that node\n",
    "            # See the corresponding MDB and add the importance of the node to the impact of that MDB\n",
    "\n",
    "            #print(node, ds['sMDiNs'][samp].edges()[edge]['Transformation'], ordered_imp_feats.loc[node])\n",
    "            Weighted_MDB_Impact[samp][ds['sMDiNs'][samp].edges()[edge][\n",
    "                'Transformation']] += ordered_imp_feats.loc[str(node)]\n",
    "\n",
    "# Weighted MDB Impact\n",
    "ds['test']['WMDBI'] = pd.DataFrame.from_dict(Weighted_MDB_Impact).replace({np.nan:0}).T\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a4b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['YD']['WMDBI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dcd566",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['HD']['test']['WMDBI']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749c2b91",
   "metadata": {},
   "source": [
    "### Re-Generate json files and HDF Store including the data matrices obtained from sMDiN analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6984c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure dir exists\n",
    "path = Path.cwd() / \"store_files\"\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "storepath = Path.cwd() / \"store_files\" / 'processed_data.h5'\n",
    "\n",
    "store = pd.HDFStore(storepath, complevel=9, complib=\"blosc:blosclz\")\n",
    "#pd.set_option('io.hdf.default_format','table')\n",
    "\n",
    "# keep json serializable values and store dataFrames in HDF store\n",
    "\n",
    "serializable = {}\n",
    "# Store in and h5 store the pandas dataframes created and in json file the rest\n",
    "# Since we have a lot of nested dicts in 'iter_fold_splits', the save and load the files back up code is a bit complex\n",
    "# Probably not the best way but functional\n",
    "# 'AA_' and 'TTS_' are used as special delimiters for the iter_fold_splits and train and test sets keys in the dict\n",
    "# Since they have the nested dicts to call on them when reading back the files\n",
    "for dskey, dataset in datasets.items():\n",
    "    serializable[dskey] = {}\n",
    "    for key, value in dataset.items():\n",
    "        #print(dskey, key)\n",
    "        if isinstance(value, pd.DataFrame):\n",
    "            storekey = dskey + '_' + key\n",
    "            #print('-----', storekey)\n",
    "            store[storekey] = value\n",
    "            serializable[dskey][key] = f\"INSTORE_{storekey}\"\n",
    "        elif key in ('MDiN', 'sMDiNs'):\n",
    "            continue\n",
    "        elif key == 'iter_fold_splits':\n",
    "            for iteration, i in value.items():\n",
    "                for group, n in i.items():\n",
    "                    for fold, j in n.items():\n",
    "                        #print(j)\n",
    "                        for treat, dfs in j.items():\n",
    "                            storekey = dskey + '_' + key + 'AA_' + str(iteration) + '_' + group + '_' + str(fold) + '_' + treat\n",
    "                            if treat == 'target':\n",
    "                                serializable[dskey][storekey] = dfs\n",
    "                            #print(df)\n",
    "                            else:\n",
    "                                store[storekey] = dfs\n",
    "                                serializable[dskey][storekey] = f\"INSTORE_{storekey}\"\n",
    "        elif key in ('train','test'):\n",
    "            for treat, dfs in value.items():\n",
    "                storekey = dskey + '_' + key + 'TTS_' + treat\n",
    "                if treat == 'target':\n",
    "                    serializable[dskey][storekey] = dfs\n",
    "                #print(df)\n",
    "                else:\n",
    "                    store[storekey] = dfs\n",
    "                    serializable[dskey][storekey] = f\"INSTORE_{storekey}\"\n",
    "        else:\n",
    "            serializable[dskey][key] = value\n",
    "store.close()\n",
    "\n",
    "\n",
    "path = path / 'processed_data.json'\n",
    "with open(path, \"w\", encoding='utf8') as write_file:\n",
    "    json.dump(serializable, write_file)\n",
    "\n",
    "    #serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6566a59b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
