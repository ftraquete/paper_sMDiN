{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Mass-Difference Networks in Metabolomics Data Analysis\n",
    "\n",
    "Notebook to support the study on the application of **Sample M**ass-**Di**fference **N**etworks as a highly specific competing form of pre-processing procedure for high-resolution metabolomics data.\n",
    "\n",
    "Mass-Difference Networks are focused into making networks from a list of masses. Each _m/z_ will represent a node. Nodes will be connected if the difference in their masses can be associated to a simple chemical reaction (enzymatic or non-enzymatic) that led to a change in the elemental composition of its metabolite.\n",
    "\n",
    "The set of mass differences used to build said networks are called a set of MDBs - Mass-Difference-based Building block.\n",
    "\n",
    "This is notebook `paper_sMDiNs_unsupervised.ipynb`\n",
    "\n",
    "\n",
    "## Organization of the Notebook\n",
    "\n",
    "- Loading up pre-processed and pre-treated datasets databases with intensity-based pre-treated data and data from sMDiNs analyses.\n",
    "- **PCA, Agglomerative Hierarchical Clustering and K-means Clustering: assessment of performence given a ground-truth of cluster assignments.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Needed Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.spatial.distance as dist\n",
    "import scipy.cluster.hierarchy as hier\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import ticker\n",
    "\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "import sklearn.cluster as skclust\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Metabolinks package\n",
    "import metabolinks as mtl\n",
    "import metabolinks.transformations as transf\n",
    "\n",
    "# Python files in the repository\n",
    "import multianalysis as ma\n",
    "from elips import plot_confidence_ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json for persistence\n",
    "import json\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of dataset records\n",
    "\n",
    "`datasets` is the global dict that holds all data sets. It is a **dict of dict's**.\n",
    "\n",
    "Each data set is **represented as a dict**.\n",
    "\n",
    "Each record has the following fields (keys):\n",
    "\n",
    "- `name`: the table/figure name of the data set\n",
    "- `source`: the biological source for each dataset\n",
    "- `mode`: the aquisition mode\n",
    "- `alignment`: the alignment used to generate the data matrix\n",
    "- `data`: the data matrix\n",
    "- `target`: the sample labels, possibly already integer encoded\n",
    "- `MDiN`: Mass-Difference Network - Not present here, only on sMDiNsAnalysis notebook\n",
    "- `<treatment name>`: transformed data matrix / network. These treatment names can be\n",
    "    - `Ionly`: missing value imputed data by 1/5 of the minimum value in each sample in the dataset, only\n",
    "    - `NGP`: normalized, glog transformed and Pareto scaled\n",
    "    - `Ionly_RF`: missing value imputed data by random forests, only\n",
    "    - `NGP_RF`: normalized, glog transformed and Pareto scaled\n",
    "    - `IDT`: `NGP_RF` or `NGP` - Intensity-based Data pre-Treatment chosen as comparison based on which of the two performed better for each dataset and each statistical method\n",
    "    - `sMDiN`: Sample Mass-Difference Networks - Not present here, only on sMDiNsAnalysis notebook\n",
    "       \n",
    "- `<sMDiN analysis name>`: data matrix from nework analysis of MDiNs - Not in this notebook\n",
    "    - `Degree`: degree analysis of each sMDiN\n",
    "    - `Betweenness`: betweenness centrality analysis of each sMDiN\n",
    "    - `Closeness`: closeness centrality of analysis of each sMDiN\n",
    "    - `MDBI`: analysis on the impact of each MDB (Mass-Difference based building-block) on building each sMDiN\n",
    "    - `GCD11`: Graphlet Correlation Distance of 11 different orbits (maximum of 4-node graphlets) between each sMDiN.\n",
    "    - `WMDBI`: an alternative calculation of MDBI using the results from the degree analysis.\n",
    "\n",
    "- `iter_fold_splits`: contains nested dicts that identify and contain each transformed training and testing groups data matrices with their respective iteration, training/test, fold number and one of the previously mentioned data pre-treatments\n",
    "- `train`: specific to the HD dataset; contains a set of the different pre-treatments and sMDin analysis mentioned and a target based on the training set defined for HD\n",
    "- `test`: specific to the HD dataset; contains a set of the different pre-treatments and sMDin analysis mentioned and a target based on the external test set defined for HD\n",
    "\n",
    "\n",
    "The keys of `datasets` may be shared with dicts holding records resulting from comparison analysis.\n",
    "\n",
    "Here are the keys (and respective names) of datasets used in this study:\n",
    "\n",
    "- GD_global2 (GDg2)\n",
    "- GD_class2 (GDc2)\n",
    "- YD (YD)\n",
    "- vitis_types (GD types)\n",
    "- HD (HD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pre-Treatment\n",
    "\n",
    "For information on the **commonly used intensity based data pre-treatments** and about the **benchmark datasets**, see notebook `paper_sMDiNs_database_prep.ipynb`.\n",
    "\n",
    "For information on the **building** and the different **network analysis methods** used for the **Sample MDiNs** and information about the Mass-Difference-based Building blocks (**MDBs**), see notebook `paper_sMDiNs_sMDiNsAnalysis.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading datasets database\n",
    "\n",
    "No need to load 'iter_fold_splits' key (or the 'train' and 'test' key of the HD dataset) for unsupervised analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the datasets are\n",
    "path = Path.cwd() / \"store_files\" / 'processed_data.json'\n",
    "storepath = Path.cwd() / \"store_files\" / 'processed_data.h5'\n",
    "with pd.HDFStore(storepath) as store:\n",
    "    \n",
    "    # Read into a dictionary not DataFrame data\n",
    "    with open(path, encoding='utf8') as read_file:\n",
    "        datasets = json.load(read_file)\n",
    "    \n",
    "    # Add DataFrame data to dict\n",
    "    for dskey, dataset in datasets.items():\n",
    "        dataset['iter_fold_splits'] = {}\n",
    "        if dskey == 'HD':\n",
    "            dataset['train'] = {}\n",
    "            dataset['test'] = {}\n",
    "        for key in dataset:\n",
    "            # Created right before\n",
    "            if 'iter_fold_splits' == key:\n",
    "                continue\n",
    "            value = dataset[key]\n",
    "            if isinstance(value, str) and value.startswith(\"INSTORE\"):\n",
    "                storekey = value.split(\"_\", 1)[1]\n",
    "                #print(storekey)\n",
    "                # Load the data from 'iter_fold_splits' carefully restoring the nested dictionaries\n",
    "                #if len(storekey.split(\"AA_\")) > 1: # This separation was made to identify the 'iter_fold_splits' data\n",
    "                #    dictkeys = (storekey.split(\"AA_\")[1]).split('_',3)\n",
    "                    # Create nested dicts\n",
    "                #    if int(dictkeys[0]) not in dataset['iter_fold_splits'].keys():\n",
    "                #        dataset['iter_fold_splits'][int(dictkeys[0])] = {}\n",
    "                #    if dictkeys[1] not in dataset['iter_fold_splits'][int(dictkeys[0])].keys():\n",
    "                #        dataset['iter_fold_splits'][int(dictkeys[0])][dictkeys[1]] = {}\n",
    "                #    if int(dictkeys[2]) not in dataset['iter_fold_splits'][int(dictkeys[0])][dictkeys[1]].keys():\n",
    "                #        dataset['iter_fold_splits'][int(dictkeys[0])][dictkeys[1]][int(dictkeys[2])] = {}\n",
    "                #    dataset['iter_fold_splits'][int(dictkeys[0])][dictkeys[1]][int(dictkeys[2])][dictkeys[3]] = store[storekey]\n",
    "                \n",
    "                # Load the data from 'train' and 'test' from HD dataset keys carefully restoring the nested dictionaries\n",
    "                #elif len(storekey.split(\"TTS_\")) > 1:\n",
    "                #    dictkeys = ((storekey.split(\"TTS_\")[0]).split('_')[-1], storekey.split(\"TTS_\")[1])#.split('_',2)\n",
    "                #    dataset[dictkeys[0]][dictkeys[1]] = store[storekey]\n",
    "                # Normal DataFrames\n",
    "                #else:\n",
    "                dataset[key] = store[storekey]\n",
    "\n",
    "            # convert colors to tuples, since they are read as lists from json file\n",
    "            elif key == 'label_colors':\n",
    "                dataset[key] = {lbl: tuple(c) for lbl, c in value.items()}\n",
    "            elif key == 'sample_colors':\n",
    "                dataset[key] = [tuple(c) for c in value]\n",
    "            #elif key.endswith('target') and key.startswith(dskey):\n",
    "            #    if len(key.split(\"AA_\")) > 1: \n",
    "            #        dictkeys = ((key.split(\"_\", 1)[1]).split(\"AA_\")[1]).split('_',3)\n",
    "            #        dataset['iter_fold_splits'][int(dictkeys[0])][dictkeys[1]][int(dictkeys[2])][dictkeys[3]] = value\n",
    "            #    else:\n",
    "            #        dictkeys = ((key.split(\"TTS_\")[0]).split('_')[-1], key.split(\"TTS_\")[1])#.split('_',2)\n",
    "            #        dataset[dictkeys[0]][dictkeys[1]] = value\n",
    "\n",
    "# Remove extra keys\n",
    "for name, ds in datasets.items():\n",
    "    keys_to_remove = [keys for keys in ds.keys() if keys.startswith(name)]\n",
    "    for key in keys_to_remove:\n",
    "        ds.pop(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['YD']['Ionly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a placeholder for the Intensity-based Data pre-Treatment (IDT)\n",
    "# Chosen for each dataset and each method based on which between NGP and NGP_RF generated the best results\n",
    "for name, ds in datasets.items():\n",
    "    ds['IDT'] = ds['NGP_RF'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemical Formula transformations (MDBs chosen)\n",
    "MDB = ['H2','CH2','CO2','O','CHOH','NCH','O(N-H-)','S','CONH','PO3H','NH3(O-)','SO3','CO', 'C2H2O', 'H2O']\n",
    "MDB_YD = ['H2','CH2','CO2','O','CHOH','NCH','O(N-H-)','S','CONH','PO3H','NH3(O-)','SO3','CO', 'C2H2O', 'H2O', \n",
    "          'C2H2O2', 'C3H4O2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colors for plots to ensure consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11 variety grapevine data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize label colors for 11 grapevine varieties\n",
    "\n",
    "colours = sns.color_palette('Blues', 3)\n",
    "colours.extend(sns.color_palette('Greens', 3))\n",
    "#colours = sns.cubehelix_palette(n_colors=6, start=2, rot=0, dark=0.2, light=.9, reverse=True)\n",
    "colours.extend(sns.color_palette('flare', 5))\n",
    "\n",
    "ordered_vitis_labels = ('CAN','RIP','ROT','RU','LAB','SYL','REG','CS','PN','RL','TRI')\n",
    "\n",
    "vitis_label_colors = {lbl: c for lbl, c in zip(ordered_vitis_labels, colours)}\n",
    "\n",
    "tab20bcols = sns.color_palette('tab20b', 20)\n",
    "tab20ccols = sns.color_palette('tab20c', 20)\n",
    "tab20cols = sns.color_palette('tab20', 20)\n",
    "tab10cols = sns.color_palette('tab10', 10)\n",
    "dark2cols = sns.color_palette('Dark2', 8)\n",
    "\n",
    "vitis_label_colors['RU'] = tab20bcols[8]\n",
    "vitis_label_colors['CAN'] = tab20ccols[5]\n",
    "vitis_label_colors['REG'] = tab10cols[3]\n",
    "\n",
    "for name in datasets:\n",
    "    if name.startswith('GD'):\n",
    "        datasets[name]['label_colors'] = vitis_label_colors\n",
    "        datasets[name]['sample_colors'] = [vitis_label_colors[lbl] for lbl in datasets[name]['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(vitis_label_colors.values())\n",
    "new_ticks = plt.xticks(range(len(ordered_vitis_labels)), ordered_vitis_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 yeast strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize label colors for 5 yeast strains\n",
    "\n",
    "colours = sns.color_palette('Set1', 5)\n",
    "yeast_classes = datasets['YD']['classes']\n",
    "yeast_label_colors = {lbl: c for lbl, c in zip(yeast_classes, colours)}\n",
    "datasets['YD']['label_colors'] = yeast_label_colors\n",
    "datasets['YD']['sample_colors'] = [yeast_label_colors[lbl] for lbl in datasets['YD']['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(yeast_label_colors.values())\n",
    "new_ticks = plt.xticks(range(len(yeast_classes)), yeast_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 classes of Vitis types (wild and _vinifera_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize label colors for 2 types of Vitis varieties\n",
    "\n",
    "colours = [vitis_label_colors['SYL'], vitis_label_colors['TRI']]\n",
    "vitis_type_classes = datasets['vitis_types']['classes']\n",
    "vitis_types_label_colors = {lbl: c for lbl, c in zip(vitis_type_classes, colours)}\n",
    "datasets['vitis_types']['label_colors'] = vitis_types_label_colors\n",
    "datasets['vitis_types']['sample_colors'] = [vitis_types_label_colors[lbl] for lbl in datasets['vitis_types']['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(datasets['vitis_types']['label_colors'].values())\n",
    "new_ticks = plt.xticks(range(len(datasets['vitis_types']['classes'])), datasets['vitis_types']['classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 HD classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize label colors for 2 HD classes\n",
    "\n",
    "colours = sns.color_palette('Set1', 2)\n",
    "hd_label_colors = {lbl: c for lbl, c in zip(datasets['HD']['classes'], colours)}\n",
    "datasets['HD']['label_colors'] = hd_label_colors\n",
    "datasets['HD']['sample_colors'] = [hd_label_colors[lbl] for lbl in datasets['HD']['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(hd_label_colors.values())\n",
    "new_ticks = plt.xticks(range(len(datasets['HD']['classes'])), datasets['HD']['classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples and respective target labels of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def styled_sample_labels(sample_names, sample_labels, label_colors):\n",
    "\n",
    "    meta_table = pd.DataFrame({'label': sample_labels,\n",
    "                               'sample': sample_names}).set_index('sample').T\n",
    "\n",
    "    def apply_label_color(val):\n",
    "        red, green, blue = label_colors[val]\n",
    "        red, green, blue = int(red*255), int(green*255), int(blue*255)   \n",
    "        hexcode = '#%02x%02x%02x' % (red, green, blue)\n",
    "        css = f'background-color: {hexcode}'\n",
    "        return css\n",
    "    \n",
    "    return meta_table.style.applymap(apply_label_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = mtl.parse_data(datasets['GD_class2']['data'], labels_loc='label')\n",
    "y = datasets['GD_class2']['target']\n",
    "label_colors = datasets['GD_class2']['label_colors']\n",
    "s = styled_sample_labels(parsed.sample_names, y, label_colors)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = mtl.parse_data(datasets['YD']['data'])\n",
    "y = datasets['YD']['target']\n",
    "label_colors = datasets['YD']['label_colors']\n",
    "s = styled_sample_labels(parsed.sample_names, y, label_colors)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = mtl.parse_data(datasets['vitis_types']['data'], labels_loc='label')\n",
    "y = datasets['vitis_types']['target']\n",
    "label_colors = datasets['vitis_types']['label_colors']\n",
    "s = styled_sample_labels(parsed.sample_names, y, label_colors)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = mtl.parse_data(datasets['HD']['data'])\n",
    "y = datasets['HD']['target']\n",
    "label_colors = datasets['HD']['label_colors']\n",
    "s = styled_sample_labels(parsed.sample_names, y, label_colors)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colors for the pre-treatments / sMDiN analysis metrics for the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize colors for the intensity-based pre-treatment and analysis metrics of sample MDiNs\n",
    "treatments = ('IDT', 'Degree', 'Betweenness', 'Closeness', 'MDBI', 'WMBDI', 'GCD11')\n",
    "\n",
    "treat_colors = tab10cols[:4]\n",
    "treat_colors.extend(tab20cols[8:10])\n",
    "treat_colors.append(tab10cols[5])\n",
    "treatment_colors = {lbl: c for lbl, c in zip(treatments, treat_colors)}\n",
    "\n",
    "sns.palplot(treatment_colors.values())\n",
    "new_ticks = plt.xticks(range(len(treatment_colors)), treatment_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA scores plots for the datasets\n",
    "\n",
    "Representation of the samples when projected in the 2 Principal Components obtained from PCA.\n",
    "\n",
    "Preliminary assessment of the extent of class’s proximity, and consequent degree of difficulty for clustering and classification methods. Greater proximity/overlap would mean a more difficult task for the methods since it would mean the classes are similar to each other or less well defined.\n",
    "\n",
    "Ellipses shown are 95% confidence ellipses for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PCA(principaldf, label_colors, components=(1,2), title=\"PCA\", ax=None):\n",
    "    \"Plot the projection of samples in the 2 main components of a PCA model.\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    loc_c1, loc_c2 = [c - 1 for c in components]\n",
    "    col_c1_name, col_c2_name = principaldf.columns[[loc_c1, loc_c2]]\n",
    "    \n",
    "    #ax.axis('equal')\n",
    "    ax.set_xlabel(f'{col_c1_name}')\n",
    "    ax.set_ylabel(f'{col_c2_name}')\n",
    "\n",
    "    unique_labels = principaldf['Label'].unique()\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        subset = principaldf[principaldf['Label']==lbl]\n",
    "        ax.scatter(subset[col_c1_name],\n",
    "                   subset[col_c2_name],\n",
    "                   s=50, color=label_colors[lbl], label=lbl)\n",
    "\n",
    "    #ax.legend(framealpha=1)\n",
    "    ax.set_title(title, fontsize=15)\n",
    "\n",
    "def plot_ellipses_PCA(principaldf, label_colors, components=(1,2),ax=None, q=None, nstd=2):\n",
    "    \"Plot confidence ellipses of a class' samples based on their projection in the 2 main components of a PCA model.\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    loc_c1, loc_c2 = [c - 1 for c in components]\n",
    "    points = principaldf.iloc[:, [loc_c1, loc_c2]]\n",
    "    \n",
    "    #ax.axis('equal')\n",
    "\n",
    "    unique_labels = principaldf['Label'].unique()\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        subset_points = points[principaldf['Label']==lbl]\n",
    "        plot_confidence_ellipse(subset_points, q, nstd, ax=ax, ec=label_colors[lbl], fc='none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1):\n",
    "        f, axs = plt.subplots(2,3, figsize=(12,8), constrained_layout=True)\n",
    "\n",
    "        for (dskey, ds), ax in zip(datasets.items(), axs.ravel()):\n",
    "            df = datasets[dskey]['Ionly']\n",
    "            tf = transf.FeatureScaler(method='standard')\n",
    "            df = tf.fit_transform(df)\n",
    "\n",
    "            ax.axis('equal')\n",
    "            principaldf = ma.compute_df_with_PCs(df, n_components=5, whiten=True, labels=datasets[dskey]['target'], return_var_ratios=False)\n",
    "\n",
    "            lcolors = datasets[dskey]['label_colors']\n",
    "            #plot_PCA(principaldf, lcolors, components=(1,2), title=datasets[dskey]['name'], ax=ax)\n",
    "            plot_PCA(principaldf, lcolors, components=(1,2), title='', ax=ax)\n",
    "            plot_ellipses_PCA(principaldf, lcolors, components=(1,2),ax=ax, q=0.95)\n",
    "\n",
    "        axs[1][2].remove()\n",
    "        axs[1][0].legend(loc='upper center', ncol=1, framealpha=1)\n",
    "        axs[1][1].legend(loc='upper center', ncol=1, framealpha=1)\n",
    "        \n",
    "        locs_YD = {'WT':(-1,-0.65),\n",
    "                   'ΔGRE3':(-0.45, 1.45),\n",
    "                   'ΔENO1':(0.7, 0.1),\n",
    "                   'ΔGLO1':(0.5, -0.5),\n",
    "                   'ΔGLO2':(0,0.7) }\n",
    "        \n",
    "        for lbl in datasets['YD']['classes']:\n",
    "            axs[0][2].text(*locs_YD[lbl], lbl, c=datasets['YD']['label_colors'][lbl])\n",
    "\n",
    "        locs_GD = {'CAN':(-1.4,-0.2),\n",
    "                       'CS':(-0.45, 2),\n",
    "                       'LAB':(-0.25, -0.2),\n",
    "                       'PN':(-1, 0.2),\n",
    "                       'REG':(1.8,0.5),\n",
    "                       'RIP':(-0.3,-0.7),\n",
    "                       'RL':(-0.5, 1.45),\n",
    "                       'ROT':(-1, -1.2),\n",
    "                       'RU':(0.5, -1),\n",
    "                       'SYL':(-0.2,0),\n",
    "                       'TRI':(0.5,0.5),}\n",
    "        \n",
    "        for lbl in datasets['GD_global2']['classes']:\n",
    "            axs[0][0].text(*locs_GD[lbl], lbl, c=datasets['GD_global2']['label_colors'][lbl])\n",
    "\n",
    "        locs_GD = {'CAN':(-0.1,-1),\n",
    "                       'CS':(-0.45, 2),\n",
    "                       'LAB':(-0.2, -0.5),\n",
    "                       'PN':(-1, 0.6),\n",
    "                       'REG':(1.8,0.35),\n",
    "                       'RIP':(-0.3,-1.7),\n",
    "                       'RL':(-0.2, 1),\n",
    "                       'ROT':(-1, -1.2),\n",
    "                       'RU':(0.9, -1),\n",
    "                       'SYL':(-1.2,-0.2),\n",
    "                       'TRI':(0.5,0.1),}\n",
    "        \n",
    "        for lbl in datasets['GD_global2']['classes']:\n",
    "            axs[0][1].text(*locs_GD[lbl], lbl, c=datasets['GD_global2']['label_colors'][lbl])\n",
    "\n",
    "        plt.show()\n",
    "        f.savefig('images/PCAs.pdf', dpi=300)\n",
    "        f.savefig('images/PCAs.jpg', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Hierarchical Cluster Analysis \n",
    "\n",
    "HCA analysis of each differently-treated datasets, results from sample MDiN analysis and corresponding dendrograms.\n",
    "\n",
    "**Euclidean distance** and **UPGMA linkage** used to build the dendrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_HCA(df, metric='euclidean', method='average'):\n",
    "    \"Performs Hierarchical Clustering Analysis of a data set with chosen linkage method and distance metric.\"\n",
    "    \n",
    "    distances = dist.pdist(df, metric=metric)\n",
    "    \n",
    "    # method is one of\n",
    "    # ward, average, centroid, single, complete, weighted, median\n",
    "    Z = hier.linkage(distances, method=method)\n",
    "\n",
    "    # Cophenetic Correlation Coefficient\n",
    "    # (see how the clustering - from hier.linkage - preserves the original distances)\n",
    "    coph = hier.cophenet(Z, distances)\n",
    "    # Baker's gamma\n",
    "    mr = ma.mergerank(Z)\n",
    "    bg = mr[mr!=0]\n",
    "\n",
    "    return {'Z': Z, 'distances': distances, 'coph': coph, 'merge_rank': mr, \"Baker's Gamma\": bg}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation of linkages, distances and cophenetics\n",
    "\n",
    "Traditional intensity-based pre-treatments\n",
    "\n",
    "- IDT - **I**ntensity-based **D**ata pre-**T**reatment\n",
    "\n",
    "Analysis metrics of the Sample MDiNs\n",
    "\n",
    "- Degree\n",
    "- Betweenness - Betweenness centrality\n",
    "- Closeness - Closeness centrality\n",
    "- MDBI - Mass-Difference based Building block Impact\n",
    "- WMDBI - Weighted Mass-Difference based Building Block Impact\n",
    "- GCD11 - Graphlet Correlation Distance using 11 non-redundant graphlet orbits (maximum of 4-node graphlets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries to contain results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCA_all = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the clusterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, ds in datasets.items():\n",
    "    HCA_all[name] = {}\n",
    "    for treat in 'NGP', 'NGP_RF', 'Degree', 'Betweenness', 'Closeness', 'MDBI', 'WMDBI', 'GCD11':\n",
    "        print(f'Performing HCA to {name} data set with treatment {treat}', end=' ...')\n",
    "        metric = 'euclidean'\n",
    "        HCA_all[name][treat] = perform_HCA(datasets[name][treat], metric=metric, method='average')\n",
    "        print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative dendogram plots - Newer\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "def color_list_to_matrix_and_cmap(colors, ind, axis=0):\n",
    "        if any(issubclass(type(x), list) for x in colors):\n",
    "            all_colors = set(itertools.chain(*colors))\n",
    "            n = len(colors)\n",
    "            m = len(colors[0])\n",
    "        else:\n",
    "            all_colors = set(colors)\n",
    "            n = 1\n",
    "            m = len(colors)\n",
    "            colors = [colors]\n",
    "        color_to_value = dict((col, i) for i, col in enumerate(all_colors))\n",
    "\n",
    "        matrix = np.array([color_to_value[c]\n",
    "                           for color in colors for c in color])\n",
    "\n",
    "        matrix = matrix.reshape((n, m))\n",
    "        matrix = matrix[:, ind]\n",
    "        if axis == 0:\n",
    "            # row-side:\n",
    "            matrix = matrix.T\n",
    "\n",
    "        cmap = mpl.colors.ListedColormap(all_colors)\n",
    "        return matrix, cmap\n",
    "\n",
    "def plot_dendogram(Z, leaf_names, label_colors, title='', ax=None, no_labels=False, labelsize=12, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    hier.dendrogram(Z, labels=leaf_names, leaf_font_size=10, above_threshold_color='0.2', orientation='left',\n",
    "                    ax=ax, **kwargs)\n",
    "    #Coloring labels\n",
    "    #ax.set_ylabel('Distance (AU)')\n",
    "    ax.set_xlabel('Distance (AU)')\n",
    "    ax.set_title(title, fontsize = 15)\n",
    "    \n",
    "    #ax.tick_params(axis='x', which='major', pad=12)\n",
    "    ax.tick_params(axis='y', which='major', labelsize=labelsize, pad=12)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    #xlbls = ax.get_xmajorticklabels()\n",
    "    xlbls = ax.get_ymajorticklabels()\n",
    "    rectimage = []\n",
    "    for lbl in xlbls:\n",
    "        col = label_colors[lbl.get_text()]\n",
    "        lbl.set_color(col)\n",
    "        #lbl.set_fontweight('bold')\n",
    "        if no_labels:\n",
    "            lbl.set_color('w')\n",
    "        rectimage.append(col)\n",
    "    \n",
    "    cols, cmap = color_list_to_matrix_and_cmap(rectimage, range(len(rectimage)), axis=0)\n",
    "\n",
    "    axins = inset_axes(ax, width=\"5%\", height=\"100%\",\n",
    "                   bbox_to_anchor=(1, 0, 1, 1),\n",
    "                   bbox_transform=ax.transAxes, loc=3, borderpad=0)\n",
    "\n",
    "    axins.pcolor(cols, cmap=cmap, edgecolors='w', linewidths=1)\n",
    "    axins.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(5, 10))\n",
    "name = 'GD_global2'\n",
    "title = f\"Data set {datasets[name]['name']}, NGP treatment\"\n",
    "plot_dendogram(HCA_all[name]['NGP']['Z'], \n",
    "               datasets[name]['target'], ax=ax,\n",
    "               label_colors=datasets[name]['label_colors'], title=title,\n",
    "               color_threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dendrograms of 4 differently-treated datasets for each of the benchmark datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 4, figsize=(14, 8), constrained_layout=True)\n",
    "    \n",
    "    name = 'GD_global2'\n",
    "    \n",
    "    for treatment, ax in zip(('NGP', 'Degree', 'MDBI', 'GCD11'), axs.ravel()):\n",
    "        plot_dendogram(HCA_all[name][treatment]['Z'], \n",
    "                       datasets[name]['target'], ax=ax,\n",
    "                       label_colors=datasets[name]['label_colors'],\n",
    "                       title=treatment, color_threshold=0)\n",
    "\n",
    "    st = f.suptitle(f\"Data set {datasets[name]['name']}\", fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 4, figsize=(14, 8), constrained_layout=True)\n",
    "    \n",
    "    name = 'GD_class2'\n",
    "      \n",
    "    for treatment, ax in zip(('NGP', 'Degree', 'MDBI', 'WMDBI'), axs.ravel()):\n",
    "        if treatment == 'NGP':\n",
    "            title = 'IDT'\n",
    "        else:\n",
    "            title = treatment\n",
    "        plot_dendogram(HCA_all[name][treatment]['Z'], \n",
    "                       datasets[name]['target'], ax=ax,\n",
    "                       label_colors=datasets[name]['label_colors'],\n",
    "                       title=title, color_threshold=0)\n",
    "\n",
    "    #st = f.suptitle(f\"Data set {datasets[name]['name']}\", fontsize=16)\n",
    "    #for letter, ax in zip('ABCDEFGHIJ', axs.ravel()):\n",
    "    #    ax.text(0.3, 0.98, letter, ha='left', va='center', fontsize=15, weight='bold',\n",
    "    #            transform=ax.transAxes,\n",
    "    #            bbox=dict(facecolor='white', alpha=0.9))\n",
    "\n",
    "    plt.show()\n",
    "    f.savefig('images/dendrosGDc2neg.pdf', dpi=300)\n",
    "    f.savefig('images/dendrosGDc2neg.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 4, figsize=(12, 4), constrained_layout=True)\n",
    "    \n",
    "    name = 'YD'\n",
    "      \n",
    "    for treatment, ax in zip(('NGP', 'Degree', 'MDBI', 'GCD11'), axs.ravel()):\n",
    "        if treatment == 'NGP':\n",
    "            title = 'IDT'\n",
    "        else:\n",
    "            title = treatment\n",
    "        plot_dendogram(HCA_all[name][treatment]['Z'], \n",
    "                       datasets[name]['target'], ax=ax,\n",
    "                       label_colors=datasets[name]['label_colors'],\n",
    "                       title=title, color_threshold=0)\n",
    "\n",
    "    st = f.suptitle('Data set YD', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 4, figsize=(14, 14), constrained_layout=True)\n",
    "    \n",
    "    name = 'HD'\n",
    "      \n",
    "    for treatment, ax in zip(('NGP_RF', 'Degree', 'MDBI', 'GCD11'), axs.ravel()):\n",
    "        plot_dendogram(HCA_all[name][treatment]['Z'], \n",
    "                       datasets[name]['target'], ax=ax,\n",
    "                       label_colors=datasets[name]['label_colors'],\n",
    "                       title=treatment, color_threshold=0)\n",
    "\n",
    "    st = f.suptitle(f'Data set {datasets[name][\"name\"]}', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Dendrogram Similarity Comparison\n",
    "\n",
    "The similarity of the dendrograms built from the differently-treated datasets of each of the benchmark datasets were compared using two correlation coefficients:\n",
    "\n",
    "#### Cophenetic Correlation Coefficient\n",
    "\n",
    "- Pearson Correlation of the matrix of cophenetic distances of two dendrograms.\n",
    "\n",
    "#### Baker's Gamma Correlation Coefficient\n",
    "\n",
    "- Use of the `mergerank` function from multianalysis.py to create a 'rank' of the iteration number two samples were linked to the same cluster. Then see Kendall Correlation between the results from 2 dendrograms according to Baker's paper or Spearman Correlation according to explanation given in the R package `dendextend`.\n",
    "\n",
    "Baker's paper: Baker FB. Stability of Two Hierarchical Grouping Techniques Case 1: Sensitivity to Data Errors. J Am Stat Assoc. 1974;69(346):440-445. doi:10.2307/2285675\n",
    "\n",
    "The information from HCA for these methods is already collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of procedure with these methods with the Negative Grapevine Dataset - GDg2-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, ds in datasets.items():\n",
    "    HCA_all[name] = {}\n",
    "    print(f'Performing HCAs to {name} data set', end=' ...')\n",
    "    for treat in 'NGP', 'NGP_RF', 'IDT', 'Degree', 'Betweenness', 'Closeness', 'MDBI', 'WMDBI', 'GCD11':\n",
    "        #print(f'Performing HCA to {name} data set with treatment {treat}', end=' ...')\n",
    "        metric = 'euclidean'\n",
    "        HCA_all[name][treat] = perform_HCA(datasets[name][treat], metric=metric, method='average')\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correation metrics\n",
    "pearsonr = stats.pearsonr\n",
    "kendalltau = stats.kendalltau\n",
    "spearmanr = stats.spearmanr\n",
    "\n",
    "table = []\n",
    "t1 = HCA_all['GD_global2']['NGP']\n",
    "t2 = HCA_all['GD_global2']['NGP_RF']\n",
    "t3 = HCA_all['GD_global2']['Degree']\n",
    "\n",
    "r, p_value = pearsonr(t1['coph'][1], t2['coph'][1])\n",
    "k, p_value_k = kendalltau(t1[\"Baker's Gamma\"], t2[\"Baker's Gamma\"])\n",
    "s, p_value_s = spearmanr(t1[\"Baker's Gamma\"], t2[\"Baker's Gamma\"])\n",
    "table.append({'Pair of samples': 'NGP Treat-NGP_RF Treat',\n",
    "              'Cophenetic (Pearson)': r,\n",
    "              '(coph) p-value': p_value,\n",
    "              \"Baker's (Kendall)\":k,\n",
    "              '(B-K) p-value': p_value_k,\n",
    "              \"Baker's (Spearman)\":s,\n",
    "              '(B-S) p-value': p_value_s,})\n",
    "\n",
    "r, p_value = pearsonr(t1['coph'][1], t3['coph'][1])\n",
    "k, p_value_k = kendalltau(t1[\"Baker's Gamma\"], t3[\"Baker's Gamma\"])\n",
    "s, p_value_s = spearmanr(t1[\"Baker's Gamma\"], t3[\"Baker's Gamma\"])\n",
    "table.append({'Pair of samples': 'NGP Treat-Degree Treat',\n",
    "              'Cophenetic (Pearson)': r,\n",
    "              '(coph) p-value': p_value,\n",
    "              \"Baker's (Kendall)\":k,\n",
    "              '(B-K) p-value': p_value_k,\n",
    "              \"Baker's (Spearman)\":s,\n",
    "              '(B-S) p-value': p_value_s,})\n",
    "\n",
    "pd.DataFrame(table).set_index('Pair of samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate all the pairwise correlations between the dendrograms\n",
    "\n",
    "Choose the set of treatment/distance metric combination to consider to calculate the pairwise correlations. These are indicated by the strings in the treatments list. The colnames list has to follow the same logic as the treatments list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names and row names for the dataframes and heatmaps\n",
    "# Collect results of HCAs\n",
    "\n",
    "colnames = ['IDT', 'Degree', 'Betweenness', 'Closeness', 'MDBI', 'WMDBI', 'GCD-11']\n",
    "treatments = ['IDT', 'Degree', 'Betweenness', 'Closeness', 'MDBI', 'WMDBI', 'GCD11']\n",
    "\n",
    "colnames = ['NGP', 'NGP_RF', 'Degree', 'Betweenness', 'Closeness', 'MDBI', 'WMDBI', 'GCD-11']\n",
    "treatments = ['NGP', 'NGP_RF', 'Degree', 'Betweenness', 'Closeness', 'MDBI', 'WMDBI', 'GCD11']\n",
    "\n",
    "# Calculation of correlation coefficient by each method\n",
    "\n",
    "def create_HCA_correlations(HCA_results, treatments, colnames):\n",
    "    n_res = len(colnames)\n",
    "    correlations = {key: np.empty((n_res, n_res)) for key in ('K', 'S', 'C', 'K_p', 'S_p', 'C_p')}\n",
    "\n",
    "    for i, treat1 in enumerate(treatments):\n",
    "        for j, treat2 in enumerate(treatments):\n",
    "            Si, Sj = HCA_results[treat1], HCA_results[treat2]\n",
    "            \n",
    "            # K - Kendall (Baker's Gamma)\n",
    "            k, p_value_k = stats.kendalltau(Si[\"Baker's Gamma\"], Sj[\"Baker's Gamma\"])\n",
    "            correlations['K'][i,j], correlations['K_p'][i,j] = k, p_value_k\n",
    "\n",
    "            # S - Spearman (Baker's Gamma)\n",
    "            s, p_value_s = stats.spearmanr(Si[\"Baker's Gamma\"], Sj[\"Baker's Gamma\"])\n",
    "            correlations['S'][i,j], correlations['S_p'][i,j] = s, p_value_s\n",
    "\n",
    "            # C - Cophenetic Correlation\n",
    "            r, p_value = stats.pearsonr(Si['coph'][1], Sj['coph'][1])\n",
    "            correlations['C'][i,j], correlations['C_p'][i,j] = r, p_value\n",
    "\n",
    "    for k in correlations:\n",
    "        correlations[k] = pd.DataFrame(correlations[k], columns=colnames, index=colnames)\n",
    "    return correlations\n",
    "\n",
    "correlations_neg = create_HCA_correlations(HCA_all['GD_global2'], treatments, colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps of the correlation coeficients\n",
    "\n",
    "As for the Baker's Gamma Correlation, the heatmaps presented will be the ones with corelation calculated with Kendall correlation (according to the original paper - Baker FB. Stability of Two Hierarchical Grouping Techniques Case 1: Sensitivity to Data Errors. J Am Stat Assoc. 1974;69(346):440-445. doi:10.2307/2285675).\n",
    "\n",
    "Although, seeing the other correlations is just a case of changing the 'C's and 'K's to 'S's based on which set of correlations you want to see in the `combineCK` function (2 cells below).\n",
    "\n",
    "Here, **two sets of heatmaps** - Baker's gamma (Kendall) correlation (upper) and cophenetic correlation (lower) - of the GDg2- or GDg2+ datasets with the IDT and the sample MDiN analysis metrics are shown.\n",
    "\n",
    "Below are the functions to build these heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_luminance(color):\n",
    "    \"\"\"Calculate the relative luminance of a color according to W3C standards\n",
    "    Parameters\n",
    "    ----------\n",
    "    color : matplotlib color or sequence of matplotlib colors\n",
    "        Hex code, rgb-tuple, or html color name.\n",
    "    Returns\n",
    "    -------\n",
    "    luminance : float(s) between 0 and 1\n",
    "    \"\"\"\n",
    "    rgb = mpl.colors.to_rgba_array(color)[:, :3]\n",
    "    rgb = np.where(rgb <= .03928, rgb / 12.92, ((rgb + .055) / 1.055) ** 2.4)\n",
    "    lum = rgb.dot([.2126, .7152, .0722])\n",
    "    try:\n",
    "        return lum.item()\n",
    "    except ValueError:\n",
    "        return lum\n",
    "\n",
    "def plot_partitioned_df_asheatmap(df, ax=None, cmap='viridis', vmin=None, vmax=None, norm=None,\n",
    "                                  partition_point=0, top_rotate=False, fontsize=14, colorbar=True):\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    values = df.values.copy()\n",
    "    #values = np.flipud(values)\n",
    "    # handle partition point\n",
    "    \n",
    "    # insert NaN column/row in values\n",
    "    values = np.insert(values, partition_point, np.nan, axis=1)\n",
    "    #values = np.insert(values, df.shape[0]- partition_point, np.nan, axis=0)\n",
    "    values = np.insert(values, partition_point, np.nan, axis=0)\n",
    "    \n",
    "    # compute and insert 2% offset\n",
    "    X = np.array(range(values.shape[0] + 1), dtype=float)\n",
    "    Y = np.array(range(values.shape[1] + 1), dtype=float)\n",
    "    offset = X[-1] * 0.02\n",
    "    \n",
    "    X[(partition_point+1):] = np.arange(float(partition_point)+offset, float(len(X)-1), 1.0)\n",
    "    Y[(partition_point+1):] = np.arange(float(partition_point)+offset, float(len(Y)-1), 1.0)\n",
    "    #Y[(len(Y)-partition_point-1):] = np.arange(float(len(Y)-partition_point-2)+offset, float(len(Y)-1), 1.0)\n",
    "\n",
    "    # draw pcolormesh\n",
    "    pm = ax.pcolormesh(X, Y, values, cmap=cmap, vmin=vmin, vmax=vmax, norm=norm)\n",
    "    ax.set_ylim(ax.get_ylim()[1], ax.get_ylim()[0])\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    \n",
    "    # handle labels\n",
    "    midpoints_x = (X[1:] - X[:-1]) / 2 + X[:-1]\n",
    "    midpoints_x = np.delete(midpoints_x, partition_point)\n",
    "    midpoints_y = (Y[1:] - Y[:-1]) / 2 + Y[:-1]\n",
    "    midpoints_y = np.delete(midpoints_y, partition_point)\n",
    "    ax.set_xticks(midpoints_x)\n",
    "    ax.set_yticks(midpoints_y)\n",
    "    ax.set_xticklabels(df.columns)\n",
    "    ax.set_yticklabels(df.index)\n",
    "    ax.tick_params(labeltop=True, labelbottom=False, labelsize=fontsize,\n",
    "                   top=False, bottom=False, left=False, right=False)\n",
    "    if top_rotate:\n",
    "        # Rotate the tick labels and set their alignment.\n",
    "        plt.setp(ax.get_xticklabels(), rotation=90, ha=\"left\", va='center', rotation_mode=\"anchor\")\n",
    "    \n",
    "    # handle annotations\n",
    "    \n",
    "    pm_colors = pm.cmap(pm.norm(pm.get_array())).reshape(values.shape[0], values.shape[1], 4)\n",
    "    mask = np.ones((values.shape[0], values.shape[1]), dtype=bool)\n",
    "    mask[:, partition_point] = False\n",
    "    mask[partition_point, :] = False\n",
    "    pm_colors = pm_colors[mask].reshape(df.shape[0], df.shape[1], 4)\n",
    "    #print(pm_colors)\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        for j in range(df.shape[1]):\n",
    "            locx = midpoints_x[j]\n",
    "            locy = midpoints_y[i]\n",
    "            # handle label color according to cell color\n",
    "            cell_color = pm_colors[i, j, :]\n",
    "            lum = relative_luminance(cell_color)\n",
    "            text_color = \".15\" if lum > .408 else \"w\"\n",
    "            annot = f'{df.iloc[i, j]:.2g}'\n",
    "            text = ax.text(locx, locy, annot, fontsize=fontsize,\n",
    "                           ha=\"center\", va=\"center\", color=text_color)\n",
    "\n",
    "    if colorbar:\n",
    "        plt.colorbar(pm)\n",
    "    return pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineCK(correlations):\n",
    "    correlations['CK'] = correlations['C'].copy()\n",
    "    # lower tringular mask\n",
    "    upper_mask = np.triu(np.ones(correlations['CK'].shape)).astype(bool)\n",
    "    correlations['CK'][upper_mask] = correlations['K']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12,10))\n",
    "\n",
    "combineCK(correlations_neg)\n",
    "\n",
    "pm = plot_partitioned_df_asheatmap(correlations_neg['CK'], ax=ax, vmin=-0.2, vmax=1,\n",
    "                                    cmap=sns.color_palette(\"rocket_r\", as_cmap=True),\n",
    "                                    partition_point=2, top_rotate=False)\n",
    "ax.text(4.25, -0.9, \"Baker's gamma (Kendall) correlation (upper) and cophenetic correlation (lower)\",\n",
    "        ha='center', fontsize=14)\n",
    "\n",
    "#f.suptitle(\"Data set GDg2-\", fontsize=16, y=1.04)\n",
    "#ax.text(1.5,-0.45, 'Traditional treatments', fontsize=14, ha='center')\n",
    "ax.text(4.65,-0.45, 'MDiN analysis metrics', fontsize=14, ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Dendrogram (HCAs) Sample Discrimination\n",
    "\n",
    "To evaluate the discrimination achieved with each HCA, 3 different metrics were used:\n",
    "\n",
    "- **Discrimination Distance** - the average of “class discrimination distance”. For each class, the discrimination distance is 0 if the class is not “correctly clustered” or it is the distance between the node that includes all the samples of the class and the next closest node (including those samples) in the agglomerative procedure, normalized by the maximum distance of any pair of nodes in the final resulting clustering.\n",
    "- **Correct Clustering Percentage** - the percentage of the classes who are correctly clustered.\n",
    "- **Correct First Cluster Percentage** - the percentage of samples whose first clustering was only with a sample(s) from its class.\n",
    "\n",
    "Correct (Class) Clustering definition - samples of a class all clustered together before any other clustering with other samples or already-formed clusters in the agglomerative procedure.\n",
    "\n",
    "Functions applied here (`dist_discrim` and `correct_1stcluster_fraction`) from multianalysis.py file of this repository with explanations of each step to calculate the different metrics.\n",
    "\n",
    "**Note**: For `vitis_types` and `HD` datasets, only the correct first cluster percentage was used, since the other metrics are susceptible to outliers which can very well come up in data with high sample number and number of samples per class such as these two datasets. Furthermore, `vitis_types` dendrograms are equal to `GD_class2` since the only difference between these data sets is the target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clustering_metrics(res_dict, labels):\n",
    "    \"\"\"Fill dict with clustering performance metrics.\"\"\"\n",
    "    \n",
    "    discrim = ma.dist_discrim(res_dict['Z'], labels, # all samples have the same order\n",
    "                              method = 'average')\n",
    "    res_dict['Average discrim dist'] = discrim[0]\n",
    "    correct = np.array(list(discrim[1].values()))\n",
    "    \n",
    "    classes = pd.unique(labels)\n",
    "    res_dict['% correct clustering'] = (100/len(classes)) * len(correct[correct>0])\n",
    "\n",
    "    # Correct First Cluster Percentage\n",
    "    res_dict['% correct 1st clustering'] = 100 * ma.correct_1stcluster_fraction(res_dict['Z'],labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrograms Discrimination Results\n",
    "\n",
    "Compute clustering metrics for the dendrograms built from the benchmark datasets treated with an intensity-based pre-treatment or with the different analysis metrics used for the sample MDiNs.\n",
    "\n",
    "\n",
    "See which of the intensity-based data pre-treatments (`NGP` or `NGP_RF`) leads to better HCA results based on the three discrimination metrics used for the work for each dataset to choose it as the `IDT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCA_performance = []\n",
    "for name, dataset in datasets.items():\n",
    "    \n",
    "    for treatment in ('NGP', 'NGP_RF'):\n",
    "        compute_clustering_metrics(HCA_all[name][treatment], datasets[name]['target'])\n",
    "        perform = {'dataset': name, 'treatment': treatment,\n",
    "                   'Discrimination Distance': HCA_all[name][treatment]['Average discrim dist'],\n",
    "                   '% correct clusters': HCA_all[name][treatment]['% correct clustering'],\n",
    "                   '% correct 1st clustering': HCA_all[name][treatment]['% correct 1st clustering']}\n",
    "        HCA_performance.append(perform)\n",
    "        \n",
    "HCA_performance = pd.DataFrame(HCA_performance)\n",
    "\n",
    "cv_dsnames = {name:datasets[name]['name'] for name in datasets}\n",
    "\n",
    "HCA_performance2 = HCA_performance.assign(dataset = HCA_performance['dataset'].map(cv_dsnames))\n",
    "\n",
    "p4 = sns.color_palette('tab20', 2)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    f, axs = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)\n",
    "    sns.barplot(x=\"dataset\", y=\"Discrimination Distance\", hue=\"treatment\", data=HCA_performance2, ax=axs[2], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"% correct clusters\", hue=\"treatment\", data=HCA_performance2, ax=axs[0], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"% correct 1st clustering\", hue=\"treatment\", data=HCA_performance2, ax=axs[1], palette=p4)\n",
    "    axs[1].legend().set_visible(False)\n",
    "    axs[2].legend().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCA_performance = []\n",
    "for name, dataset in datasets.items():\n",
    "    \n",
    "    for treatment in ('IDT', 'Degree', 'Betweenness', 'Closeness', 'MDBI', 'WMDBI', 'GCD11'):\n",
    "        # Choice of the IDT for each dataset\n",
    "        if treatment == 'IDT':\n",
    "            if name in ('YD', 'HD'):\n",
    "                compute_clustering_metrics(HCA_all[name]['NGP_RF'], datasets[name]['target'])\n",
    "                perform = {'dataset': name, 'treatment': treatment,\n",
    "                   'Discrimination Distance': HCA_all[name]['NGP_RF']['Average discrim dist'],\n",
    "                   '% correct clusters': HCA_all[name]['NGP_RF']['% correct clustering'],\n",
    "                   '% correct 1st clustering': HCA_all[name]['NGP_RF']['% correct 1st clustering']}\n",
    "            else:\n",
    "                compute_clustering_metrics(HCA_all[name]['NGP'], datasets[name]['target'])\n",
    "                perform = {'dataset': name, 'treatment': treatment,\n",
    "                   'Discrimination Distance': HCA_all[name]['NGP']['Average discrim dist'],\n",
    "                   '% correct clusters': HCA_all[name]['NGP']['% correct clustering'],\n",
    "                   '% correct 1st clustering': HCA_all[name]['NGP']['% correct 1st clustering']}\n",
    "        \n",
    "        # Data matrices of sMDiNs network analyses\n",
    "        else:\n",
    "            compute_clustering_metrics(HCA_all[name][treatment], datasets[name]['target'])\n",
    "            perform = {'dataset': name, 'treatment': treatment,\n",
    "                       'Discrimination Distance': HCA_all[name][treatment]['Average discrim dist'],\n",
    "                       '% correct clusters': HCA_all[name][treatment]['% correct clustering'],\n",
    "                       '% correct 1st clustering': HCA_all[name][treatment]['% correct 1st clustering']}\n",
    "        HCA_performance.append(perform)\n",
    "        \n",
    "HCA_performance = pd.DataFrame(HCA_performance)\n",
    "\n",
    "cv_dsnames = {name:datasets[name]['name'] for name in datasets}\n",
    "\n",
    "HCA_performance2 = HCA_performance.assign(dataset = HCA_performance['dataset'].map(cv_dsnames))\n",
    "HCA_performance2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = treat_colors\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    f, axs = plt.subplots(3, 1, figsize=(12, 12), constrained_layout=True)\n",
    "    sns.barplot(x=\"dataset\", y=\"Discrimination Distance\", hue=\"treatment\", data=HCA_performance2, ax=axs[0], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"% correct clusters\", hue=\"treatment\", data=HCA_performance2, ax=axs[1], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"% correct 1st clustering\", hue=\"treatment\", data=HCA_performance2, ax=axs[2], palette=p4)\n",
    "    axs[1].legend().set_visible(False)\n",
    "    axs[2].legend().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering Analysis\n",
    "\n",
    "K-means clustering analysis was applied by using the appropriate functions of the scikit-learn as done in the following cells.\n",
    "\n",
    "#### K-means clustering was applied to all the datasets obtained for each of the benchmark datasets\n",
    "\n",
    "The number of clusters chosen was equal to the amount of groups. Apart from this, default parameters were used.\n",
    "\n",
    "K-means clustering analysis has an intrinsically random side to it depending on the starting position of the clusters centroids and due to the existence of local minima. Due to this randomness, the algorithm was repeated 15 (n) times and the result with the least inertia (greater minimization of the objective function - sum of squared distances of the samples to the cluster centroids) was retained (best 10% of results, in this case, only the best).\n",
    "\n",
    "To evaluate the discrimination achieved with each K-means Clustering, 3 different metrics were used:\n",
    "\n",
    "- **Discrimination Distance** (for K-means clustering, identical idea to HCA)\n",
    "- **Correct Clustering Percentage** (for K-means clustering, identical idea to HCA)\n",
    "- **Adjusted Rand Index** (calculated by scikit-learn - `adjusted_rand_index`) - proportion of sample pairs which are correctly clustered or correctly not clustered, adjusted for the expected percentage of samples which would be in those situations randomly.\n",
    "\n",
    "The function `Kmeans_discrim` from multianalysis.py was applied to calculate these metrics with explanations of each step to calculate the different metrics.\n",
    "\n",
    "Correct clustering definition - K-means Cluster contains all and only the samples of a single class (stricter definition than in HCA). Samples of a class can all be together in a cluster, but if another sample (of another class) is present, the class is not correctly clustered. Thus, the Correct Clustering Percentage is expected to be lower in this case.\n",
    "\n",
    "In this case, the distances are calculated by the distance between different cluster centroids.\n",
    "\n",
    "**Note**: As before, for `vitis_types` and `HD` datasets, only the Adjusted Rand Index was used, since the other metrics are susceptible to outliers which can very well come up in data with high sample number and number of samples per class such as these two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_KMeans(dataset, treatment, iter_num=150, best_fraction=0.1):\n",
    "    \"Perform K-means Clustering Analysis and calculate discrimination evaluation metrics.\"\n",
    "    \n",
    "    sample_labels = datasets[dataset]['target']\n",
    "    n_classes = len(pd.unique(sample_labels))\n",
    "    \n",
    "    df = datasets[dataset][treatment]\n",
    "    \n",
    "    discrim = ma.Kmeans_discrim(df, sample_labels,\n",
    "                                method='average', \n",
    "                                iter_num=iter_num,\n",
    "                                best_fraction=best_fraction)\n",
    "\n",
    "    \n",
    "    # Lists for the results of the best k-means clustering\n",
    "    average = []\n",
    "    correct = []\n",
    "    rand = []\n",
    "    \n",
    "    for j in discrim:\n",
    "        global_disc_dist, disc_dists, rand_index, SSE = discrim[j]\n",
    "        \n",
    "        # Average of discrimination distances\n",
    "        average.append(global_disc_dist) \n",
    "        \n",
    "        # Correct Clustering Percentages\n",
    "        all_correct = np.array(list(disc_dists.values()))\n",
    "        correct.append(len(all_correct[all_correct>0]))\n",
    "        \n",
    "        # Adjusted Rand Index\n",
    "        rand.append(rand_index) \n",
    "    \n",
    "    return{'dataset': dataset,\n",
    "           'treatment': treatment,\n",
    "           'Discrimination Distance': np.median(average),\n",
    "           '% correct clusters':np.median(correct)*100/n_classes,\n",
    "           'Adjusted Rand Index': np.median(rand)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See which of the intensity-based data pre-treatments (`NGP` or `NGP_RF`) leads to better K-means clustering results based on the three discrimination metrics used for the work for each dataset to choose it as the `IDT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num=20\n",
    "\n",
    "KMeans_all = []\n",
    "\n",
    "for dsname in ('GD_global2', 'GD_class2', 'YD', 'vitis_types', 'HD'):\n",
    "    for treatment in ('NGP', 'NGP_RF'):\n",
    "        print(f'performing KMeans on {dsname} with treatment {treatment}' , end=' ...')\n",
    "        KMeans_all.append(perform_KMeans(dsname, treatment, iter_num=iter_num))\n",
    "        print('done!')\n",
    "        \n",
    "KMeans_all = pd.DataFrame(KMeans_all)\n",
    "\n",
    "cv_dsnames = {name:datasets[name]['name'] for name in datasets}\n",
    "KMeans_all2 = KMeans_all.assign(dataset = KMeans_all['dataset'].map(cv_dsnames))\n",
    "\n",
    "KMeans_all2\n",
    "\n",
    "p4 = sns.color_palette('tab20', 2)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    f, axs = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)\n",
    "    sns.barplot(x=\"dataset\", y=\"Discrimination Distance\", hue=\"treatment\", data=KMeans_all2, ax=axs[2], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"% correct clusters\", hue=\"treatment\", data=KMeans_all2, ax=axs[0], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"Adjusted Rand Index\", hue=\"treatment\", data=KMeans_all2, ax=axs[1], palette=p4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(16)\n",
    "\n",
    "iter_num=20\n",
    "\n",
    "KMeans_all = []\n",
    "\n",
    "for dsname in ('GD_global2', 'GD_class2', 'YD', 'vitis_types', 'HD'):\n",
    "    # Choice of the IDT for each dataset\n",
    "    datasets[dsname]['IDT'] = datasets[dsname]['NGP']\n",
    "    if dsname == 'HD':\n",
    "        datasets[dsname]['IDT'] = datasets[dsname]['NGP_RF']\n",
    "    for treatment in ('IDT', 'Degree', 'Betweenness', 'Closeness', 'MDBI', 'WMDBI', 'GCD11'):\n",
    "        print(f'performing KMeans on {dsname} with treatment {treatment}' , end=' ...')\n",
    "        KMeans_all.append(perform_KMeans(dsname, treatment, iter_num=iter_num))\n",
    "        print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_all = pd.DataFrame(KMeans_all)\n",
    "\n",
    "cv_dsnames = {name:datasets[name]['name'] for name in datasets}\n",
    "KMeans_all2 = KMeans_all.assign(dataset = KMeans_all['dataset'].map(cv_dsnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_all2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = treat_colors\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    f, axs = plt.subplots(3, 1, figsize=(12, 12), constrained_layout=True)\n",
    "    #for ax in axs.ravel():\n",
    "        #ax.tick_params(labelsize=14)\n",
    "        #ax.xaxis.label.set_size(16)\n",
    "        #ax.axhspan(-0.5, 3.5, color='red', alpha=0.2)\n",
    "        #ax.axhspan(3.55, 7.5, color='darkblue', alpha=0.2)\n",
    "        #ax.axhspan(7.55, 11.5, color='red', alpha=0.2)\n",
    "        #ax.axhspan(11.55, 16, color='darkblue', alpha=0.2)\n",
    "    sns.barplot(x=\"dataset\", y=\"Discrimination Distance\", hue=\"treatment\", data=KMeans_all2, ax=axs[0], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"% correct clusters\", hue=\"treatment\", data=KMeans_all2, ax=axs[1], palette=p4)\n",
    "    sns.barplot(x=\"dataset\", y=\"Adjusted Rand Index\", hue=\"treatment\", data=KMeans_all2, ax=axs[2], palette=p4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Clustering performance\n",
    "\n",
    "HCA and K-means Clustering results combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = treat_colors\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.4):\n",
    "        #f, axs = plt.subplots(2, 3, figsize=(16, 8), constrained_layout=True)\n",
    "        \n",
    "        fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "        gs = plt.GridSpec(2,14)\n",
    "\n",
    "        ax1 = fig.add_subplot(gs[0,:4])\n",
    "        ax2 = fig.add_subplot(gs[0,4:8])\n",
    "        ax3 = fig.add_subplot(gs[0,8:])\n",
    "        \n",
    "        ax4 = fig.add_subplot(gs[1,:4])\n",
    "        ax5 = fig.add_subplot(gs[1,4:8])\n",
    "        ax6 = fig.add_subplot(gs[1,8:]) \n",
    "        \n",
    "        \n",
    "        sns.barplot(x=\"dataset\", y=\"Discrimination Distance\", hue=\"treatment\", data=HCA_performance2.loc[:20], ax=ax2, palette=p4)\n",
    "        sns.barplot(x=\"dataset\", y=\"% correct clusters\", hue=\"treatment\", data=HCA_performance2.loc[:20], ax=ax1, palette=p4)\n",
    "        sns.barplot(x=\"dataset\", y=\"% correct 1st clustering\", hue=\"treatment\", data=HCA_performance2, ax=ax3, palette=p4)\n",
    "\n",
    "        sns.barplot(x=\"dataset\", y=\"Discrimination Distance\", hue=\"treatment\", data=KMeans_all2.loc[:20], ax=ax5, palette=p4)\n",
    "        sns.barplot(x=\"dataset\", y=\"% correct clusters\", hue=\"treatment\", data=KMeans_all2.loc[:20], ax=ax4, palette=p4)\n",
    "        sns.barplot(x=\"dataset\", y=\"Adjusted Rand Index\", hue=\"treatment\", data=KMeans_all2, ax=ax6, palette=p4)\n",
    "        for ax in (ax1, ax2, ax3, ax4, ax5, ax6):\n",
    "            ax.set_ylim(0,105)\n",
    "            ax.xaxis.label.set_visible(False)\n",
    "            ax.legend().set_visible(False)\n",
    "            ax.tick_params(axis='x', which='major', labelsize=14)\n",
    "\n",
    "        ax2.legend(bbox_to_anchor=(1,1), loc=\"upper right\", framealpha=1, fontsize=13, ncol=2)\n",
    "        \n",
    "        ax2.set_ylim(0,1.05)\n",
    "        ax5.set_ylim(0,1.05)\n",
    "        ax6.set_ylim(0,1.05)\n",
    "        \n",
    "        #for letter, ax in zip('ABCDEFGHIJ', axs.ravel()):\n",
    "        #    ax.text(0.05, 0.9, letter, ha='left', va='center', fontsize=16, weight='bold',\n",
    "        #            transform=ax.transAxes,\n",
    "        #            bbox=dict(facecolor='white', edgecolor='white', alpha=0.9))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        fig.savefig('images/clust_performance.pdf' , dpi=300)\n",
    "        fig.savefig('images/clust_performance.jpg' , dpi=300)\n",
    "        #fig.savefig('images/clust_performance.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap just to make a colorbar useful for an image in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "tf = transf.FeatureScaler(method='standard')\n",
    "df = tf.fit_transform(datasets['YD']['MDBI'])\n",
    "\n",
    "myColors = ((0/255, 0/255, 255/255), (51/255, 51/255, 255/255), (102/255, 102/255, 255/255),\n",
    "           (153/255, 153/255, 255/255), (204/255, 204/255, 255/255), (255/255, 204/255, 204/255),\n",
    "           (255/255, 153/255, 204/255), (255/255, 102/255, 102/255), (255/255, 51/255, 51/255),\n",
    "           (255/255, 0/255, 0/255), (204/255, 0/255, 0/255), (102/255, 0/255, 0/255))\n",
    "cmap = LinearSegmentedColormap.from_list('Custom', myColors, len(myColors))\n",
    "\n",
    "g = sns.heatmap(df.T, cmap = cmap, vmin=0, vmax=12)\n",
    "\n",
    "# Manually specify colorbar labelling after it's been generated\n",
    "colorbar = g.collections[0].colorbar\n",
    "colorbar.set_ticks([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.5, 11.5])\n",
    "colorbar.set_ticklabels([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], fontsize=18)\n",
    "\n",
    "g.set_title('Node\\nDegree', fontsize=18)\n",
    "\n",
    "#fig.savefig('images/colorbar.jpg' , dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
